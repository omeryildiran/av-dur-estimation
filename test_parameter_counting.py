#!/usr/bin/env python3
"""
Test script to validate parameter counting and AIC calculations
"""

import numpy as np
import pandas as pd
from monteCarloClass import OmerMonteCarlo

def test_parameter_counting():
    """Test parameter counting for all model configurations"""
    
    # Create dummy data for testing
    np.random.seed(42)
    n_trials = 100
    dummy_data = pd.DataFrame({
        'deltaDurS': np.random.uniform(-0.3, 0.3, n_trials),
        'audNoise': np.random.choice([0.1, 1.2], n_trials),
        'standardDur': np.full(n_trials, 0.5),
        'conflictDur': np.random.choice([-0.25, -0.17, -0.08, 0, 0.08, 0.17, 0.25], n_trials),
        'unbiasedVisualStandardDur': np.random.uniform(0.2, 0.8, n_trials),
        'unbiasedVisualTestDur': np.random.uniform(0.2, 0.8, n_trials),
        'testDurS': np.random.uniform(0.2, 0.8, n_trials),
        'chose_test': np.random.binomial(1, 0.5, n_trials),
        'chose_standard': np.random.binomial(1, 0.5, n_trials),
        'responses': np.ones(n_trials)
    })
    
    print("=" * 80)
    print("PARAMETER COUNTING VALIDATION TEST")
    print("=" * 80)
    
    test_configs = [
        # (modelName, sharedLambda, freeP_c, expected_params)
        ("gaussian", True, False, 7),       # [Œª, œÉa1, œÉv, pc, œÉa2, t_min, t_max]
        ("gaussian", False, False, 9),      # [Œª, œÉa1, œÉv, pc, œÉa2, Œª2, Œª3, t_min, t_max]
        ("gaussian", True, True, 8),        # [Œª, œÉa1, œÉv, pc1, œÉa2, pc2, t_min, t_max]
        ("gaussian", False, True, 10),      # [Œª, œÉa1, œÉv, pc1, œÉa2, Œª2, Œª3, pc2, t_min, t_max]
        ("lognorm", True, False, 7),        # Same as gaussian
        ("lognorm", False, False, 9),       # Same as gaussian
        ("fusionOnly", True, False, 6),     # [Œª, œÉa1, œÉv, œÉa2, t_min, t_max] (no p_c)
        ("fusionOnly", False, False, 6),    # [Œª, œÉa1, œÉv, œÉa2, t_min, t_max] (no p_c, no Œª2,Œª3)
        ("fusionOnlyLogNorm", True, False, 6), # Same as fusionOnly
        ("probabilityMatching", True, False, 7), # Same as gaussian
        ("probabilityMatchingLogNorm", False, True, 10), # Same as gaussian
    ]
    
    all_passed = True
    
    for model_name, shared_lambda, free_pc, expected_count in test_configs:
        try:
            # Create Monte Carlo fitter instance
            mc = OmerMonteCarlo(
                dummy_data,
                intensityVar='deltaDurS',
                sensoryVar='audNoise',
                standardVar='standardDur',
                conflictVar='conflictDur'
            )
            
            # Set configuration
            mc.modelName = model_name
            mc.sharedLambda = shared_lambda
            mc.freeP_c = free_pc
            
            # Get parameter count
            actual_count = mc.getActualParameterCount()
            
            # Test result
            if actual_count == expected_count:
                status = "‚úÖ PASS"
            else:
                status = "‚ùå FAIL"
                all_passed = False
            
            print(f"{status} | {model_name:20} | sharedŒª={shared_lambda:5} | freePC={free_pc:5} | Expected={expected_count:2} | Actual={actual_count:2}")
            
        except Exception as e:
            print(f"‚ùå FAIL | {model_name:20} | ERROR: {e}")
            all_passed = False
    
    print("=" * 80)
    if all_passed:
        print("üéâ ALL PARAMETER COUNTING TESTS PASSED!")
    else:
        print("‚ö†Ô∏è  SOME PARAMETER COUNTING TESTS FAILED!")
    print("=" * 80)
    
    return all_passed

def test_aic_calculation():
    """Test AIC calculation with actual vs len(params)"""
    
    print("\nTesting AIC calculation consistency...")
    
    # Create a simple test case
    np.random.seed(42)
    dummy_data = pd.DataFrame({
        'deltaDurS': [-0.1, 0, 0.1],
        'audNoise': [0.1, 1.2, 0.1],
        'standardDur': [0.5, 0.5, 0.5],
        'conflictDur': [0, 0, 0.1],
        'unbiasedVisualStandardDur': [0.5, 0.5, 0.6],
        'unbiasedVisualTestDur': [0.4, 0.5, 0.6],
        'testDurS': [0.4, 0.5, 0.6],
        'chose_test': [0, 1, 1],
        'chose_standard': [1, 0, 0],
        'responses': [1, 1, 1]
    })
    
    mc = OmerMonteCarlo(dummy_data)
    mc.modelName = "fusionOnly"  # Should have 6 parameters
    mc.sharedLambda = True
    mc.freeP_c = False
    
    # Test parameter arrays of different lengths
    test_param_arrays = [
        np.array([0.1, 0.5, 0.5, 0.8, 0.2, 1.0]),  # Correct length (6)
        np.array([0.1, 0.5, 0.5, 0.5, 0.8, 0.2, 1.0]),  # Wrong length (7)
    ]
    
    for i, params in enumerate(test_param_arrays):
        actual_count = mc.getActualParameterCount()
        array_length = len(params)
        
        print(f"Test {i+1}: Array length={array_length}, Actual parameters={actual_count}")
        
        if array_length == actual_count:
            print(f"  ‚úÖ Consistent: Would use correct parameter count for AIC")
        else:
            print(f"  ‚ö†Ô∏è  Inconsistent: Array has {array_length} params, but model needs {actual_count}")
            print(f"     OLD AIC calculation would use {array_length} (WRONG)")
            print(f"     NEW AIC calculation would use {actual_count} (CORRECT)")

if __name__ == "__main__":
    # Run tests
    test_parameter_counting()
    test_aic_calculation()
    
    print("\n" + "="*80)
    print("SUMMARY OF FIXES IMPLEMENTED:")
    print("="*80)
    print("1. ‚úÖ Fixed parameter bounds (broader ranges, avoid boundary issues)")
    print("2. ‚úÖ Fixed initial parameter generation (consistent with bounds)")
    print("3. ‚úÖ Improved numerical stability in likelihood calculation")
    print("4. ‚úÖ Enhanced optimization with multiple methods")
    print("5. ‚úÖ Added comprehensive parameter validation")
    print("6. ‚úÖ Fixed AIC calculation to use actual parameter count")
    print("7. ‚úÖ Added configuration validation")
    print("8. ‚úÖ Improved error handling and debugging")
    print("="*80)