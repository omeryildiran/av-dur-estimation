{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520a384b",
   "metadata": {},
   "source": [
    "# nSimul Optimization Test\n",
    "\n",
    "This notebook systematically tests different nSimul values (50, 100, 250, 500, 1000) for Monte Carlo simulations to determine the optimal simulation count that provides good model fits without overfitting.\n",
    "\n",
    "**Testing Participant:** oy  \n",
    "**Objective:** Find the sweet spot between computational efficiency and model accuracy\n",
    "\n",
    "## Overview\n",
    "- Test how nSimul affects model fitting quality\n",
    "- Detect signs of overfitting with higher simulation counts\n",
    "- Assess parameter stability and convergence\n",
    "- Evaluate computational cost vs. benefit trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cda5e",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "Import necessary libraries and load the 'oy' participant data for testing different Monte Carlo simulation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9789727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "# Custom modules\n",
    "import loadData\n",
    "import monteCarloClass\n",
    "import fitSaver\n",
    "import loadResults\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f488127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total trials before cleaning\n",
      ": 2171\n",
      "uniqueSensory: [1.2 0.1] \n",
      " uniqueStandard: [0.5] \n",
      " uniqueConflict: [np.float64(-0.25), np.float64(-0.17), np.float64(-0.08), np.float64(0.0), np.float64(0.08), np.float64(0.17), np.float64(0.25)]\n",
      "total trials after cleaning: 2171\n",
      "üìä Data loaded: oy_all\n",
      "   Shape: (2171, 57)\n",
      "   Unique test durations: 65 values\n",
      "   Range: 0.025 - 0.953 seconds\n",
      "   Unique conflicts: [np.float64(-0.25), np.float64(-0.17), np.float64(-0.08), np.float64(0.0), np.float64(0.08), np.float64(0.17), np.float64(0.25)]\n",
      "   Unique noise levels: [np.float64(0.1), np.float64(1.2)]\n",
      "\n",
      "üîç Data validation:\n",
      "   Total trials: 2171\n",
      "   Missing values: 930\n",
      "   Response column: True\n",
      "\n",
      "üìã Data preview:\n",
      "   testDurS  standardDur  conflictDur  audNoise  chose_test\n",
      "0   0.02507          0.5         0.25       1.2           0\n",
      "1   0.02507          0.5        -0.25       1.2           0\n",
      "2   0.02507          0.5        -0.17       1.2           0\n",
      "3   0.95255          0.5         0.08       1.2           1\n",
      "4   0.10862          0.5         0.00       1.2           0\n"
     ]
    }
   ],
   "source": [
    "# Load the 'oy' participant data\n",
    "data, dataName = loadData.loadData(\"oy_all.csv\")\n",
    "\n",
    "print(f\"üìä Data loaded: {dataName}\")\n",
    "print(f\"   Shape: {data.shape}\")\n",
    "print(f\"   Unique test durations: {len(data['testDurS'].unique())} values\")\n",
    "print(f\"   Range: {data['testDurS'].min():.3f} - {data['testDurS'].max():.3f} seconds\")\n",
    "print(f\"   Unique conflicts: {sorted(data['conflictDur'].unique())}\")\n",
    "print(f\"   Unique noise levels: {sorted(data['audNoise'].unique())}\")\n",
    "\n",
    "# Quick data validation\n",
    "print(f\"\\nüîç Data validation:\")\n",
    "print(f\"   Total trials: {len(data)}\")\n",
    "print(f\"   Missing values: {data.isnull().sum().sum()}\")\n",
    "print(f\"   Response column: {'chose_test' in data.columns}\")\n",
    "\n",
    "# Preview first few rows\n",
    "print(f\"\\nüìã Data preview:\")\n",
    "print(data[['testDurS', 'standardDur', 'conflictDur', 'audNoise', 'chose_test']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e08943",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Class Configuration\n",
    "\n",
    "Initialize the OmerMonteCarlo class and set up basic configuration parameters that will remain constant across different nSimul tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration:\n",
      "   nSimul values to test: [50, 200, 500, 1000]\n",
      "   Intensity variable: deltaDurS\n",
      "   Sensory variable: audNoise\n",
      "Created logDurRatio variable: range [-2.993, 0.645]\n",
      "  ‚Üí This represents log(test/standard) for Weber's law compliance\n",
      "‚úì Configuration validated: gaussian, sharedLambda=False, freeP_c=False\n",
      "‚úÖ Base fitter configured:\n",
      "   Model: lognorm\n",
      "   nStart: 1\n",
      "   freeP_c: False\n",
      "   sharedLambda: False\n",
      "   Grouped data conditions: 674\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters that will remain constant\n",
    "INTENSITY_VAR = \"deltaDurS\"\n",
    "SENSORY_VAR = \"audNoise\"\n",
    "STANDARD_VAR = \"standardDur\"\n",
    "CONFLICT_VAR = \"conflictDur\"\n",
    "VISUAL_STANDARD_VAR = \"unbiasedVisualStandardDur\"\n",
    "VISUAL_TEST_VAR = \"unbiasedVisualTestDur\"\n",
    "AUDIO_TEST_VAR = \"testDurS\"\n",
    "\n",
    "# nSimul values to test\n",
    "NSIMUL_VALUES = [ 50, 200, 500, 1000]\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   nSimul values to test: {NSIMUL_VALUES}\")\n",
    "print(f\"   Intensity variable: {INTENSITY_VAR}\")\n",
    "print(f\"   Sensory variable: {SENSORY_VAR}\")\n",
    "\n",
    "# Create base Monte Carlo fitter configuration\n",
    "def create_base_fitter(data, dataName):\n",
    "    \"\"\"Create a base Monte Carlo fitter with standard configuration\"\"\"\n",
    "    mc_fitter = monteCarloClass.OmerMonteCarlo(data)\n",
    "    \n",
    "    # Fixed configuration across all tests\n",
    "    mc_fitter.nStart = 1  # Keep low for speed but ensure some robustness\n",
    "    mc_fitter.optimizationMethod = \"bads\"  # Use scipy for consistency\n",
    "    mc_fitter.modelName = \"lognorm\"  # Use lognorm model for testing\n",
    "    mc_fitter.integrationMethod = \"analytical\"  \n",
    "    mc_fitter.freeP_c = False  # Shared p_c across conditions\n",
    "    mc_fitter.sharedLambda = False  # Separate lambda for different conflicts\n",
    "    mc_fitter.dataName = dataName.split('_')[0]  # Clean data name\n",
    "    \n",
    "    print(f\"‚úÖ Base fitter configured:\")\n",
    "    print(f\"   Model: {mc_fitter.modelName}\")\n",
    "    print(f\"   nStart: {mc_fitter.nStart}\")\n",
    "    print(f\"   freeP_c: {mc_fitter.freeP_c}\")\n",
    "    print(f\"   sharedLambda: {mc_fitter.sharedLambda}\")\n",
    "    print(f\"   Grouped data conditions: {len(mc_fitter.groupedData)}\")\n",
    "    \n",
    "    return mc_fitter\n",
    "\n",
    "# Create base fitter\n",
    "base_fitter = create_base_fitter(data, dataName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee20bfb",
   "metadata": {},
   "source": [
    "## 3. Parameter Fitting with Different nSimul Values\n",
    "\n",
    "Systematically fit the causal inference model using nSimul values of 50, 100, 250, 500, and 1000, storing fitted parameters and log-likelihoods for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b3291ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting systematic nSimul testing...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results for each nSimul value\n",
    "fitting_results = {}\n",
    "\n",
    "# Function to fit model with specific nSimul\n",
    "def fit_with_nsimul(nSimul, base_fitter, run_id=\"\"):\n",
    "    \"\"\"Fit model with specific nSimul value and record all metrics\"\"\"\n",
    "    print(f\"\\nüîÑ Fitting with nSimul = {nSimul} {run_id}\")\n",
    "    \n",
    "    # Create a copy of the base fitter to avoid interference\n",
    "    fitter = deepcopy(base_fitter)\n",
    "    fitter.nSimul = nSimul\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Fit the model\n",
    "        fitted_params = fitter.fitCausalInferenceMonteCarlo(fitter.groupedData)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fit_time = time.time() - start_time\n",
    "        log_likelihood = -fitter.nLLMonteCarloCausal(fitted_params, fitter.groupedData)\n",
    "        n_params = fitter.getActualParameterCount()\n",
    "        n_data_points = len(fitter.groupedData)\n",
    "        \n",
    "        # Calculate AIC and BIC\n",
    "        aic = 2 * n_params - 2 * log_likelihood\n",
    "        bic = np.log(n_data_points) * n_params - 2 * log_likelihood\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'nSimul': nSimul,\n",
    "            'fitted_params': fitted_params,\n",
    "            'log_likelihood': log_likelihood,\n",
    "            'aic': aic,\n",
    "            'bic': bic,\n",
    "            'n_params': n_params,\n",
    "            'n_data_points': n_data_points,\n",
    "            'fit_time': fit_time,\n",
    "            'fitter': fitter,  # Store the fitter for later analysis\n",
    "            'success': True,\n",
    "            'run_id': run_id\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Success! LL={log_likelihood:.2f}, AIC={aic:.2f}, Time={fit_time:.1f}s\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {str(e)}\")\n",
    "        return {\n",
    "            'nSimul': nSimul,\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'fit_time': time.time() - start_time,\n",
    "            'run_id': run_id\n",
    "        }\n",
    "\n",
    "print(\"üöÄ Starting systematic nSimul testing...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ad7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Fitting with nSimul = 50 \n",
      "Data bounds: t_min=0.025, t_max=0.953\n",
      "=== DEBUGGING FITTING COMPONENTS ===\n",
      "Data bounds: t_min=0.025, t_max=0.953\n",
      "Grouped data shape: (674, 11)\n",
      "Configuration: sharedLambda=False, freeP_c=False\n",
      "Model: lognorm\n",
      "Expected parameter length: 9\n",
      "Test params length: 9, values: [0.1 0.5 0.5 0.5 0.8 0.1 0.1 0.2 1. ]\n",
      "‚úì Parameter extraction tested for 3 conditions\n",
      "Test likelihood: 1179.556564100998\n",
      "=== ALL COMPONENTS TESTED SUCCESSFULLY ===\n",
      "Fitting with shared p_c parameter across SNR conditions.\n",
      "Bounds shape: (9, 2)\n",
      "t_min bounds: [0.  0.4]\n",
      "t_max bounds: [ 1.047805 10.      ]\n",
      "Testing likelihood function with reasonable parameters...\n",
      "Test likelihood evaluation: 1190.804875800434\n",
      "\n",
      "Starting 1 optimization attempts using 'scipy'...\n",
      "Model is lognorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Attempts:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m total_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nSimul \u001b[38;5;129;01min\u001b[39;00m NSIMUL_VALUES:\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfit_with_nsimul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnSimul\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_fitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     fitting_results[nSimul] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m      8\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m total_start_time\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mfit_with_nsimul\u001b[0;34m(nSimul, base_fitter, run_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     fitted_params \u001b[38;5;241m=\u001b[39m \u001b[43mfitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitCausalInferenceMonteCarlo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupedData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-omerulfaruk97@gmail.com/My Drive/MyReposDrive/obsidian_Notes/Landy Omer Re 1/av-dur-estimation/monteCarloClass.py:902\u001b[0m, in \u001b[0;36mOmerMonteCarlo.fitCausalInferenceMonteCarlo\u001b[0;34m(self, groupedData)\u001b[0m\n\u001b[1;32m    892\u001b[0m     method_result \u001b[38;5;241m=\u001b[39m minimize(\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLLMonteCarloCausal,\n\u001b[1;32m    894\u001b[0m         x0\u001b[38;5;241m=\u001b[39mx0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m         options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mftol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-9\u001b[39m}\n\u001b[1;32m    899\u001b[0m     )\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;66;03m# Powell method (doesn't support bounds directly)\u001b[39;00m\n\u001b[0;32m--> 902\u001b[0m     method_result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnLLMonteCarloCausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroupedData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# Check if this method gave better results\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(method_result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m method_result\u001b[38;5;241m.\u001b[39mfun \u001b[38;5;241m<\u001b[39m best_method_ll:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mathmod/lib/python3.11/site-packages/scipy/optimize/_minimize.py:722\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    719\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_neldermead(fun, x0, args, callback, bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m    720\u001b[0m                                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpowell\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 722\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_powell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    724\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mathmod/lib/python3.11/site-packages/scipy/optimize/_optimize.py:3479\u001b[0m, in \u001b[0;36m_minimize_powell\u001b[0;34m(func, x0, args, callback, bounds, xtol, ftol, maxiter, maxfev, disp, direc, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   3475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(lower_bound \u001b[38;5;241m>\u001b[39m x0) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(x0 \u001b[38;5;241m>\u001b[39m upper_bound):\n\u001b[1;32m   3476\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial guess is not within the specified bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3477\u001b[0m                       OptimizeWarning, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m-> 3479\u001b[0m fval \u001b[38;5;241m=\u001b[39m squeeze(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3480\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mathmod/lib/python3.11/site-packages/scipy/optimize/_optimize.py:526\u001b[0m, in \u001b[0;36m_wrap_scalar_function_maxfun_validation.<locals>.function_wrapper\u001b[0;34m(x, *wrapper_args)\u001b[0m\n\u001b[1;32m    524\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# A copy of x is sent to the user function (gh13740)\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrapper_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# Ideally, we'd like to a have a true scalar returned from f(x). For\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# backwards-compatibility, also allow np.array([1.3]),\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# np.array([[1.3]]) etc.\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-omerulfaruk97@gmail.com/My Drive/MyReposDrive/obsidian_Notes/Landy Omer Re 1/av-dur-estimation/monteCarloClass.py:671\u001b[0m, in \u001b[0;36mOmerMonteCarlo.nLLMonteCarloCausal\u001b[0;34m(self, params, groupedData)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Calculate the probability of choosing the test duration being longer than the standard duration\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 671\u001b[0m     p_test_longer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobTestLonger_vectorized_mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrueStims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_av_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_av_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(p_test_longer) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(p_test_longer):\n\u001b[1;32m    673\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1e10\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-omerulfaruk97@gmail.com/My Drive/MyReposDrive/obsidian_Notes/Landy Omer Re 1/av-dur-estimation/monteCarloClass.py:542\u001b[0m, in \u001b[0;36mOmerMonteCarlo.probTestLonger_vectorized_mc\u001b[0;34m(self, trueStims, sigma_av_a, sigma_av_v, p_c, lambda_, t_min, t_max)\u001b[0m\n\u001b[1;32m    540\u001b[0m     m_a_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(S_a_t), scale\u001b[38;5;241m=\u001b[39msigma_av_a, size\u001b[38;5;241m=\u001b[39mnSimul)\n\u001b[1;32m    541\u001b[0m     m_v_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(S_v_t), scale\u001b[38;5;241m=\u001b[39msigma_av_v, size\u001b[38;5;241m=\u001b[39mnSimul)\n\u001b[0;32m--> 542\u001b[0m     est_standard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausalInference_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_a_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_v_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_av_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_av_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_min\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_max\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m     est_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausalInference_vectorized(m_a_t, m_v_t, sigma_av_a, sigma_av_v, p_c, np\u001b[38;5;241m.\u001b[39mlog(t_min), np\u001b[38;5;241m.\u001b[39mlog(t_max))\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# === Probability Matching Model for Log-Space === #\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-omerulfaruk97@gmail.com/My Drive/MyReposDrive/obsidian_Notes/Landy Omer Re 1/av-dur-estimation/monteCarloClass.py:447\u001b[0m, in \u001b[0;36mOmerMonteCarlo.causalInference_vectorized\u001b[0;34m(self, m_a, m_v, sigma_a, sigma_v, p_c, t_min, t_max)\u001b[0m\n\u001b[1;32m    445\u001b[0m fused_S_av \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusionAV_vectorized(m_a, m_v, sigma_a, sigma_v)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Calculate likelihoods\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m post_C1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior_C1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# convert back to linear scale if measurements are in log scale\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Note: logLinearMismatch measurements are already in linear scale (generated via exp(log-normal))\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Only lognorm needs conversion since measurements are in log space\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodelName \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlognorm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-omerulfaruk97@gmail.com/My Drive/MyReposDrive/obsidian_Notes/Landy Omer Re 1/av-dur-estimation/monteCarloClass.py:423\u001b[0m, in \u001b[0;36mOmerMonteCarlo.posterior_C1\u001b[0;34m(self, m_a, m_v, sigma_a, sigma_v, p_c, t_min, t_max)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mposterior_C1\u001b[39m(\u001b[38;5;28mself\u001b[39m,m_a,m_v,sigma_a,\n\u001b[1;32m    419\u001b[0m                                 sigma_v, p_c,\n\u001b[1;32m    420\u001b[0m                                 t_min,t_max):\n\u001b[1;32m    421\u001b[0m \n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m#likelihoods\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     L1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL_C1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    424\u001b[0m     L2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_C2(m_a, m_v, sigma_a, sigma_v, t_min, t_max)  \u001b[38;5;66;03m# Fixed: consistent parameter order\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# posterior with numerical stability\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-omerulfaruk97@gmail.com/My Drive/MyReposDrive/obsidian_Notes/Landy Omer Re 1/av-dur-estimation/monteCarloClass.py:381\u001b[0m, in \u001b[0;36mOmerMonteCarlo.L_C1\u001b[0;34m(self, m_a, m_v, sigma_a, sigma_v, t_min, t_max)\u001b[0m\n\u001b[1;32m    378\u001b[0m sigma_c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(sigma_c_sq)\n\u001b[1;32m    379\u001b[0m mu_c \u001b[38;5;241m=\u001b[39m (m_a \u001b[38;5;241m/\u001b[39m sigma_a\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m m_v \u001b[38;5;241m/\u001b[39m sigma_v\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m sigma_a\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m sigma_v\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 381\u001b[0m hi_cdf \u001b[38;5;241m=\u001b[39m \u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmu_c\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43msigma_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m lo_cdf \u001b[38;5;241m=\u001b[39m norm\u001b[38;5;241m.\u001b[39mcdf((t_min\u001b[38;5;241m-\u001b[39mmu_c)\u001b[38;5;241m/\u001b[39msigma_c)\n\u001b[1;32m    384\u001b[0m expo \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m(m_a\u001b[38;5;241m-\u001b[39mm_v)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(sigma_a\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39msigma_v\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mathmod/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:2083\u001b[0m, in \u001b[0;36mrv_continuous.cdf\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m   2080\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output[()]\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m-> 2083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;124;03m    Cumulative distribution function of the given RV.\u001b[39;00m\n\u001b[1;32m   2086\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2103\u001b[0m \n\u001b[1;32m   2104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m     args, loc, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_args(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit models with each nSimul value\n",
    "total_start_time = time.time()\n",
    "\n",
    "for nSimul in NSIMUL_VALUES:\n",
    "    result = fit_with_nsimul(nSimul, base_fitter)\n",
    "    fitting_results[nSimul] = result\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "\n",
    "print(f\"\\nüèÅ All fitting completed in {total_time:.1f} seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Quick summary of results\n",
    "print(f\"\\nüìä Fitting Summary:\")\n",
    "successful_fits = [k for k, v in fitting_results.items() if v['success']]\n",
    "failed_fits = [k for k, v in fitting_results.items() if not v['success']]\n",
    "\n",
    "print(f\"   ‚úÖ Successful fits: {len(successful_fits)}/{len(NSIMUL_VALUES)}\")\n",
    "if successful_fits:\n",
    "    print(f\"   üìà nSimul values: {successful_fits}\")\n",
    "    \n",
    "    # Show key metrics for successful fits\n",
    "    for nSimul in successful_fits:\n",
    "        r = fitting_results[nSimul]\n",
    "        print(f\"   üìã nSimul={nSimul}: LL={r['log_likelihood']:.2f}, AIC={r['aic']:.2f}, Time={r['fit_time']:.1f}s\")\n",
    "\n",
    "if failed_fits:\n",
    "    print(f\"   ‚ùå Failed fits: {failed_fits}\")\n",
    "    for nSimul in failed_fits:\n",
    "        print(f\"      nSimul={nSimul}: {fitting_results[nSimul]['error']}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Average time per fit: {np.mean([r['fit_time'] for r in fitting_results.values()]):.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db4295",
   "metadata": {},
   "source": [
    "## 4. Model Performance Comparison\n",
    "\n",
    "Compare log-likelihood, AIC, and BIC values across different nSimul settings to assess model fit quality and identify potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison DataFrame\n",
    "performance_data = []\n",
    "\n",
    "for nSimul in successful_fits:\n",
    "    r = fitting_results[nSimul]\n",
    "    performance_data.append({\n",
    "        'nSimul': nSimul,\n",
    "        'log_likelihood': r['log_likelihood'],\n",
    "        'AIC': r['aic'],\n",
    "        'BIC': r['bic'],\n",
    "        'fit_time': r['fit_time'],\n",
    "        'n_params': r['n_params']\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "\n",
    "print(\"üìä Model Performance Comparison\")\n",
    "print(\"=\"*50)\n",
    "print(performance_df.round(3))\n",
    "\n",
    "# Calculate relative performance metrics\n",
    "if len(performance_df) > 1:\n",
    "    # Calculate differences from best (lowest AIC)\n",
    "    best_aic = performance_df['AIC'].min()\n",
    "    performance_df['delta_AIC'] = performance_df['AIC'] - best_aic\n",
    "    \n",
    "    # Calculate differences from best (highest LL)\n",
    "    best_ll = performance_df['log_likelihood'].max()\n",
    "    performance_df['delta_LL'] = performance_df['log_likelihood'] - best_ll\n",
    "    \n",
    "    print(f\"\\nüìà Relative Performance (compared to best):\")\n",
    "    print(performance_df[['nSimul', 'delta_AIC', 'delta_LL', 'fit_time']].round(3))\n",
    "    \n",
    "    # Performance trends\n",
    "    print(f\"\\nüéØ Performance Trends:\")\n",
    "    print(f\"   Best AIC: nSimul = {performance_df.loc[performance_df['AIC'].idxmin(), 'nSimul']}\")\n",
    "    print(f\"   Best LL: nSimul = {performance_df.loc[performance_df['log_likelihood'].idxmax(), 'nSimul']}\")\n",
    "    print(f\"   Fastest fit: nSimul = {performance_df.loc[performance_df['fit_time'].idxmin(), 'nSimul']}\")\n",
    "    \n",
    "    # Check for improvement saturation\n",
    "    if len(performance_df) >= 3:\n",
    "        ll_improvements = np.diff(performance_df.sort_values('nSimul')['log_likelihood'].values)\n",
    "        aic_improvements = -np.diff(performance_df.sort_values('nSimul')['AIC'].values)  # Negative because lower AIC is better\n",
    "        \n",
    "        print(f\"\\nüìâ Improvement Analysis:\")\n",
    "        print(f\"   LL improvements: {ll_improvements.round(3)}\")\n",
    "        print(f\"   AIC improvements: {aic_improvements.round(3)}\")\n",
    "        \n",
    "        # Detect diminishing returns\n",
    "        if len(ll_improvements) >= 2:\n",
    "            recent_improvement = np.mean(ll_improvements[-2:])  # Last 2 improvements\n",
    "            early_improvement = np.mean(ll_improvements[:2])   # First 2 improvements\n",
    "            \n",
    "            if recent_improvement < early_improvement * 0.3:\n",
    "                print(f\"   ‚ö†Ô∏è  Diminishing returns detected: Recent improvements ({recent_improvement:.3f}) much smaller than early improvements ({early_improvement:.3f})\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ Still improving: Recent improvements ({recent_improvement:.3f}) vs early ({early_improvement:.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Need at least 2 successful fits for comparison analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe59e1",
   "metadata": {},
   "source": [
    "## 5. Overfitting Detection Analysis\n",
    "\n",
    "Analyze parameter variance, cross-validation performance, and stability metrics to detect signs of overfitting with higher nSimul values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting detection through multiple fits with same nSimul\n",
    "print(\"üîç Overfitting Detection Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test stability by running multiple fits with the highest nSimul values\n",
    "high_nsimul_values = [val for val in NSIMUL_VALUES if val >= 250]\n",
    "n_stability_runs = 3  # Number of repeated fits per nSimul for stability testing\n",
    "\n",
    "stability_results = {}\n",
    "\n",
    "print(f\"Running stability tests for high nSimul values: {high_nsimul_values}\")\n",
    "print(f\"Number of runs per nSimul: {n_stability_runs}\")\n",
    "\n",
    "for nSimul in high_nsimul_values:\n",
    "    print(f\"\\nüîÑ Testing stability for nSimul = {nSimul}\")\n",
    "    \n",
    "    stability_results[nSimul] = []\n",
    "    \n",
    "    for run in range(n_stability_runs):\n",
    "        result = fit_with_nsimul(nSimul, base_fitter, f\"(stability run {run+1})\")\n",
    "        if result['success']:\n",
    "            stability_results[nSimul].append(result)\n",
    "        \n",
    "# Analyze stability\n",
    "print(f\"\\nüìä Stability Analysis:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for nSimul in high_nsimul_values:\n",
    "    if len(stability_results[nSimul]) > 1:\n",
    "        results = stability_results[nSimul]\n",
    "        \n",
    "        # Extract metrics\n",
    "        lls = [r['log_likelihood'] for r in results]\n",
    "        aics = [r['aic'] for r in results]\n",
    "        times = [r['fit_time'] for r in results]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        ll_mean = np.mean(lls)\n",
    "        ll_std = np.std(lls)\n",
    "        aic_mean = np.mean(aics)\n",
    "        aic_std = np.std(aics)\n",
    "        time_mean = np.mean(times)\n",
    "        time_std = np.std(times)\n",
    "        \n",
    "        print(f\"\\nnSimul = {nSimul} ({len(results)} successful runs):\")\n",
    "        print(f\"   Log-Likelihood: {ll_mean:.3f} ¬± {ll_std:.3f} (CV: {ll_std/abs(ll_mean)*100:.1f}%)\")\n",
    "        print(f\"   AIC: {aic_mean:.3f} ¬± {aic_std:.3f} (CV: {aic_std/aic_mean*100:.1f}%)\")\n",
    "        print(f\"   Fit Time: {time_mean:.1f} ¬± {time_std:.1f} seconds\")\n",
    "        \n",
    "        # Overfitting indicators\n",
    "        ll_cv = ll_std / abs(ll_mean) * 100\n",
    "        aic_cv = aic_std / aic_mean * 100\n",
    "        \n",
    "        print(f\"   üìà Stability Metrics:\")\n",
    "        if ll_cv > 5:\n",
    "            print(f\"      ‚ö†Ô∏è  High LL variability ({ll_cv:.1f}%) - possible overfitting\")\n",
    "        else:\n",
    "            print(f\"      ‚úÖ Good LL stability ({ll_cv:.1f}%)\")\n",
    "            \n",
    "        if aic_cv > 5:\n",
    "            print(f\"      ‚ö†Ô∏è  High AIC variability ({aic_cv:.1f}%) - possible overfitting\")\n",
    "        else:\n",
    "            print(f\"      ‚úÖ Good AIC stability ({aic_cv:.1f}%)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"\\nnSimul = {nSimul}: Insufficient successful runs for stability analysis\")\n",
    "\n",
    "# Cross-validation style analysis by using different random seeds\n",
    "print(f\"\\nüé≤ Monte Carlo Variance Analysis:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if len(successful_fits) >= 2:\n",
    "    # Calculate Monte Carlo standard errors for different nSimul values\n",
    "    print(\"Analyzing how Monte Carlo variance decreases with nSimul...\")\n",
    "    \n",
    "    # Theoretical expectation: MC variance should decrease as 1/sqrt(nSimul)\n",
    "    # So standard error should decrease as 1/sqrt(nSimul)\n",
    "    \n",
    "    for nSimul in successful_fits:\n",
    "        fitter = fitting_results[nSimul]['fitter']\n",
    "        \n",
    "        # Quick MC variance estimate: run likelihood calculation multiple times\n",
    "        test_params = fitting_results[nSimul]['fitted_params']\n",
    "        ll_samples = []\n",
    "        \n",
    "        for mc_run in range(10):  # Small sample for speed\n",
    "            ll = -fitter.nLLMonteCarloCausal(test_params, fitter.groupedData)\n",
    "            ll_samples.append(ll)\n",
    "        \n",
    "        mc_std = np.std(ll_samples)\n",
    "        mc_mean = np.mean(ll_samples)\n",
    "        \n",
    "        print(f\"   nSimul = {nSimul}: MC std = {mc_std:.4f}, Relative std = {mc_std/abs(mc_mean)*100:.3f}%\")\n",
    "    \n",
    "    # Theoretical vs observed scaling\n",
    "    if len(successful_fits) >= 3:\n",
    "        print(f\"\\nüìê Theoretical Scaling Check:\")\n",
    "        mc_stds = []\n",
    "        nsimul_vals = []\n",
    "        \n",
    "        for nSimul in sorted(successful_fits):\n",
    "            fitter = fitting_results[nSimul]['fitter']\n",
    "            test_params = fitting_results[nSimul]['fitted_params']\n",
    "            \n",
    "            ll_samples = []\n",
    "            for mc_run in range(10):\n",
    "                ll = -fitter.nLLMonteCarloCausal(test_params, fitter.groupedData)\n",
    "                ll_samples.append(ll)\n",
    "            \n",
    "            mc_stds.append(np.std(ll_samples))\n",
    "            nsimul_vals.append(nSimul)\n",
    "        \n",
    "        # Check if std decreases approximately as 1/sqrt(nSimul)\n",
    "        relative_scaling = np.array(mc_stds) * np.sqrt(nsimul_vals)\n",
    "        scaling_cv = np.std(relative_scaling) / np.mean(relative_scaling) * 100\n",
    "        \n",
    "        print(f\"   Expected scaling factor (std * sqrt(nSimul)): {relative_scaling}\")\n",
    "        print(f\"   Scaling consistency (CV): {scaling_cv:.1f}%\")\n",
    "        \n",
    "        if scaling_cv < 30:\n",
    "            print(f\"   ‚úÖ Good adherence to theoretical scaling\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Poor scaling consistency - may indicate numerical issues\")\n",
    "\n",
    "else:\n",
    "    print(\"Insufficient successful fits for Monte Carlo variance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758359b",
   "metadata": {},
   "source": [
    "## 6. Parameter Stability Assessment\n",
    "\n",
    "Examine how fitted parameters (lambda, sigma_av_a, sigma_av_v, p_c) change with different nSimul values and assess convergence stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter stability analysis\n",
    "print(\"üéØ Parameter Stability Assessment\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract parameters for each nSimul value\n",
    "param_data = []\n",
    "\n",
    "for nSimul in successful_fits:\n",
    "    result = fitting_results[nSimul]\n",
    "    fitter = result['fitter']\n",
    "    params = result['fitted_params']\n",
    "    \n",
    "    # Extract parameters for a representative condition (low noise, no conflict)\n",
    "    test_snr = 0.1  # Low noise condition\n",
    "    test_conflict = 0.0  # No conflict condition\n",
    "    \n",
    "    try:\n",
    "        Œª, œÉa, œÉv, pc, t_min, t_max = fitter.getParamsCausal(params, test_snr, test_conflict)\n",
    "        \n",
    "        param_data.append({\n",
    "            'nSimul': nSimul,\n",
    "            'lambda': Œª,\n",
    "            'sigma_av_a': œÉa,\n",
    "            'sigma_av_v': œÉv,\n",
    "            'p_c': pc,\n",
    "            't_min': t_min,\n",
    "            't_max': t_max,\n",
    "            'all_params': params\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not extract parameters for nSimul={nSimul}: {e}\")\n",
    "\n",
    "param_df = pd.DataFrame(param_data)\n",
    "\n",
    "if len(param_df) > 1:\n",
    "    print(\"üìä Parameter Values Across nSimul:\")\n",
    "    print(param_df[['nSimul', 'lambda', 'sigma_av_a', 'sigma_av_v', 'p_c', 't_min', 't_max']].round(4))\n",
    "    \n",
    "    # Calculate parameter stability metrics\n",
    "    print(f\"\\nüìà Parameter Stability Analysis:\")\n",
    "    key_params = ['lambda', 'sigma_av_a', 'sigma_av_v', 'p_c']\n",
    "    \n",
    "    for param in key_params:\n",
    "        values = param_df[param].values\n",
    "        param_mean = np.mean(values)\n",
    "        param_std = np.std(values)\n",
    "        param_cv = param_std / abs(param_mean) * 100\n",
    "        param_range = np.max(values) - np.min(values)\n",
    "        \n",
    "        print(f\"   {param}:\")\n",
    "        print(f\"      Mean: {param_mean:.4f}\")\n",
    "        print(f\"      Std: {param_std:.4f}\")\n",
    "        print(f\"      CV: {param_cv:.1f}%\")\n",
    "        print(f\"      Range: {param_range:.4f}\")\n",
    "        \n",
    "        # Stability assessment\n",
    "        if param_cv < 10:\n",
    "            print(f\"      ‚úÖ Very stable parameter\")\n",
    "        elif param_cv < 25:\n",
    "            print(f\"      ‚ö†Ô∏è  Moderately stable parameter\")\n",
    "        else:\n",
    "            print(f\"      ‚ùå Unstable parameter - high variability\")\n",
    "    \n",
    "    # Check for convergence patterns\n",
    "    print(f\"\\nüéØ Convergence Pattern Analysis:\")\n",
    "    \n",
    "    for param in key_params:\n",
    "        values = param_df.sort_values('nSimul')[param].values\n",
    "        nsimul_sorted = param_df.sort_values('nSimul')['nSimul'].values\n",
    "        \n",
    "        # Calculate differences between consecutive nSimul values\n",
    "        param_diffs = np.abs(np.diff(values))\n",
    "        \n",
    "        # Check if differences are decreasing (sign of convergence)\n",
    "        if len(param_diffs) >= 2:\n",
    "            early_diff = np.mean(param_diffs[:len(param_diffs)//2])\n",
    "            late_diff = np.mean(param_diffs[len(param_diffs)//2:])\n",
    "            \n",
    "            print(f\"   {param}: Early changes = {early_diff:.4f}, Late changes = {late_diff:.4f}\")\n",
    "            \n",
    "            if late_diff < early_diff * 0.5:\n",
    "                print(f\"      ‚úÖ Converging - parameter stabilizing with higher nSimul\")\n",
    "            elif late_diff > early_diff * 2:\n",
    "                print(f\"      ‚ùå Diverging - parameter becoming less stable\")\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è  Unclear convergence pattern\")\n",
    "    \n",
    "    # Correlation between nSimul and parameters\n",
    "    print(f\"\\nüìä nSimul-Parameter Correlations:\")\n",
    "    \n",
    "    for param in key_params:\n",
    "        correlation = np.corrcoef(param_df['nSimul'], param_df[param])[0, 1]\n",
    "        print(f\"   {param} vs nSimul: r = {correlation:.3f}\")\n",
    "        \n",
    "        if abs(correlation) > 0.7:\n",
    "            direction = \"increasing\" if correlation > 0 else \"decreasing\"\n",
    "            print(f\"      ‚ö†Ô∏è  Strong correlation - parameter {direction} with nSimul\")\n",
    "        elif abs(correlation) > 0.3:\n",
    "            print(f\"      ‚ö†Ô∏è  Moderate correlation with nSimul\")\n",
    "        else:\n",
    "            print(f\"      ‚úÖ Parameter independent of nSimul\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Insufficient successful fits for parameter stability analysis\")\n",
    "\n",
    "# Additional analysis: Check if parameters are hitting bounds\n",
    "print(f\"\\nüö® Boundary Analysis:\")\n",
    "\n",
    "if len(param_df) > 0:\n",
    "    for nSimul in param_df['nSimul']:\n",
    "        result = fitting_results[nSimul]\n",
    "        fitter = result['fitter']\n",
    "        params = result['all_params']\n",
    "        \n",
    "        # Check for parameter values near bounds (typical bounds for our model)\n",
    "        bound_warnings = []\n",
    "        \n",
    "        # Lambda should be between 0 and 1\n",
    "        if any(p < 0.01 or p > 0.99 for p in params[:1]):  # First param is lambda\n",
    "            bound_warnings.append(\"lambda near bounds\")\n",
    "            \n",
    "        # Sigma values should be positive but not too large\n",
    "        sigma_params = [params[1], params[2], params[4]]  # sigma_av_a_1, sigma_av_v, sigma_av_a_2\n",
    "        if any(p < 0.05 or p > 2.0 for p in sigma_params):\n",
    "            bound_warnings.append(\"sigma parameters near bounds\")\n",
    "            \n",
    "        # p_c should be between 0 and 1\n",
    "        if len(params) > 3:\n",
    "            pc_params = [params[3]]  # p_c\n",
    "            if fitter.freeP_c and len(params) > 7:\n",
    "                pc_params.append(params[7])  # Second p_c if free\n",
    "            \n",
    "            if any(p < 0.01 or p > 0.99 for p in pc_params):\n",
    "                bound_warnings.append(\"p_c near bounds\")\n",
    "        \n",
    "        if bound_warnings:\n",
    "            print(f\"   nSimul = {nSimul}: ‚ö†Ô∏è  {', '.join(bound_warnings)}\")\n",
    "        else:\n",
    "            print(f\"   nSimul = {nSimul}: ‚úÖ All parameters within reasonable bounds\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No successful fits to analyze boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071cd59",
   "metadata": {},
   "source": [
    "## 7. Computational Efficiency Analysis\n",
    "\n",
    "Measure and compare fitting times for different nSimul values to assess the computational cost-benefit trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational efficiency analysis\n",
    "print(\"‚è±Ô∏è  Computational Efficiency Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create efficiency dataframe\n",
    "efficiency_data = []\n",
    "\n",
    "for nSimul in successful_fits:\n",
    "    result = fitting_results[nSimul]\n",
    "    \n",
    "    # Basic efficiency metrics\n",
    "    fit_time = result['fit_time']\n",
    "    log_likelihood = result['log_likelihood']\n",
    "    \n",
    "    # Time per simulation\n",
    "    time_per_simul = fit_time / nSimul\n",
    "    \n",
    "    # Efficiency metrics: improvement per unit time\n",
    "    base_ll = min([fitting_results[ns]['log_likelihood'] for ns in successful_fits])\n",
    "    ll_improvement = log_likelihood - base_ll\n",
    "    efficiency_ratio = ll_improvement / fit_time if fit_time > 0 else 0\n",
    "    \n",
    "    efficiency_data.append({\n",
    "        'nSimul': nSimul,\n",
    "        'fit_time': fit_time,\n",
    "        'time_per_simul': time_per_simul,\n",
    "        'log_likelihood': log_likelihood,\n",
    "        'll_improvement': ll_improvement,\n",
    "        'efficiency_ratio': efficiency_ratio,\n",
    "        'speedup_factor': 1.0  # Will calculate relative to baseline\n",
    "    })\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_data)\n",
    "\n",
    "# Calculate speedup factors (relative to slowest)\n",
    "if len(efficiency_df) > 1:\n",
    "    max_time = efficiency_df['fit_time'].max()\n",
    "    efficiency_df['speedup_factor'] = max_time / efficiency_df['fit_time']\n",
    "\n",
    "print(\"üìä Computational Efficiency Metrics:\")\n",
    "print(efficiency_df[['nSimul', 'fit_time', 'time_per_simul', 'll_improvement', 'efficiency_ratio']].round(4))\n",
    "\n",
    "# Scaling analysis\n",
    "print(f\"\\nüìà Computational Scaling Analysis:\")\n",
    "\n",
    "if len(efficiency_df) >= 3:\n",
    "    # Fit scaling relationship: time ~ nSimul^Œ±\n",
    "    log_nsimul = np.log(efficiency_df['nSimul'])\n",
    "    log_time = np.log(efficiency_df['fit_time'])\n",
    "    \n",
    "    # Linear regression in log space\n",
    "    coeffs = np.polyfit(log_nsimul, log_time, 1)\n",
    "    scaling_exponent = coeffs[0]\n",
    "    \n",
    "    print(f\"   Observed scaling: Time ‚àù nSimul^{scaling_exponent:.2f}\")\n",
    "    \n",
    "    if 0.9 <= scaling_exponent <= 1.1:\n",
    "        print(f\"   ‚úÖ Linear scaling - as expected for Monte Carlo\")\n",
    "    elif scaling_exponent > 1.1:\n",
    "        print(f\"   ‚ö†Ô∏è  Super-linear scaling - possible inefficiency\")\n",
    "    else:\n",
    "        print(f\"   ü§î Sub-linear scaling - unexpected but good!\")\n",
    "    \n",
    "    # Theoretical vs actual time predictions\n",
    "    print(f\"\\nüéØ Time Prediction Analysis:\")\n",
    "    \n",
    "    # Predict time for next nSimul values\n",
    "    future_nsimul = [1500, 2000, 5000]\n",
    "    \n",
    "    for future_n in future_nsimul:\n",
    "        predicted_time = np.exp(coeffs[1]) * (future_n ** scaling_exponent)\n",
    "        print(f\"   Predicted time for nSimul={future_n}: {predicted_time:.1f} seconds ({predicted_time/60:.1f} minutes)\")\n",
    "\n",
    "# Cost-benefit analysis\n",
    "print(f\"\\nüí∞ Cost-Benefit Analysis:\")\n",
    "\n",
    "if len(efficiency_df) >= 2:\n",
    "    sorted_df = efficiency_df.sort_values('nSimul')\n",
    "    \n",
    "    print(f\"   Efficiency ranking (LL improvement per second):\")\n",
    "    efficiency_ranking = efficiency_df.sort_values('efficiency_ratio', ascending=False)\n",
    "    \n",
    "    for i, (_, row) in enumerate(efficiency_ranking.iterrows()):\n",
    "        efficiency_score = row['efficiency_ratio']\n",
    "        if efficiency_score > 0:\n",
    "            print(f\"   {i+1}. nSimul={int(row['nSimul'])}: {efficiency_score:.4f} LL/sec\")\n",
    "        else:\n",
    "            print(f\"   {i+1}. nSimul={int(row['nSimul'])}: No improvement\")\n",
    "    \n",
    "    # Find optimal nSimul based on different criteria\n",
    "    print(f\"\\nüéØ Optimization Recommendations:\")\n",
    "    \n",
    "    # Best absolute performance\n",
    "    best_ll_idx = efficiency_df['log_likelihood'].idxmax()\n",
    "    best_ll_nsimul = efficiency_df.loc[best_ll_idx, 'nSimul']\n",
    "    \n",
    "    # Best efficiency\n",
    "    best_eff_idx = efficiency_df['efficiency_ratio'].idxmax()\n",
    "    best_eff_nsimul = efficiency_df.loc[best_eff_idx, 'nSimul']\n",
    "    \n",
    "    # Fastest\n",
    "    fastest_idx = efficiency_df['fit_time'].idxmin()\n",
    "    fastest_nsimul = efficiency_df.loc[fastest_idx, 'nSimul']\n",
    "    \n",
    "    print(f\"   Best absolute performance: nSimul = {int(best_ll_nsimul)}\")\n",
    "    print(f\"   Most efficient (LL/time): nSimul = {int(best_eff_nsimul)}\")\n",
    "    print(f\"   Fastest fitting: nSimul = {int(fastest_nsimul)}\")\n",
    "    \n",
    "    # Sweet spot analysis\n",
    "    print(f\"\\nüçØ Sweet Spot Analysis:\")\n",
    "    \n",
    "    # Look for point where efficiency starts decreasing significantly\n",
    "    sorted_eff = efficiency_df.sort_values('nSimul')\n",
    "    \n",
    "    if len(sorted_eff) >= 3:\n",
    "        eff_diffs = np.diff(sorted_eff['efficiency_ratio'].values)\n",
    "        nsimul_vals = sorted_eff['nSimul'].values[1:]  # Match length with diffs\n",
    "        \n",
    "        # Find where efficiency improvement drops below threshold\n",
    "        diminishing_returns_threshold = 0.1  # 10% of max efficiency improvement\n",
    "        max_eff_improvement = max(eff_diffs) if len(eff_diffs) > 0 else 0\n",
    "        \n",
    "        sweet_spot_candidates = []\n",
    "        for i, (diff, nsimul) in enumerate(zip(eff_diffs, nsimul_vals)):\n",
    "            if diff < diminishing_returns_threshold * max_eff_improvement:\n",
    "                # This is where diminishing returns start\n",
    "                sweet_spot_candidates.append(sorted_eff.iloc[i]['nSimul'])\n",
    "        \n",
    "        if sweet_spot_candidates:\n",
    "            sweet_spot = int(min(sweet_spot_candidates))\n",
    "            print(f\"   üí´ Sweet spot (before diminishing returns): nSimul = {sweet_spot}\")\n",
    "        else:\n",
    "            print(f\"   üìà No clear diminishing returns detected - may benefit from higher nSimul\")\n",
    "\n",
    "# Memory and resource considerations\n",
    "print(f\"\\nüíæ Resource Considerations:\")\n",
    "\n",
    "for nSimul in successful_fits:\n",
    "    memory_estimate = nSimul * 8 * 4 / 1024 / 1024  # Rough estimate: nSimul * 8 bytes * 4 variables / MB\n",
    "    print(f\"   nSimul={nSimul}: ~{memory_estimate:.1f} MB memory per likelihood calculation\")\n",
    "\n",
    "# Practical recommendations\n",
    "print(f\"\\nüéØ Practical Recommendations:\")\n",
    "\n",
    "if len(efficiency_df) >= 2:\n",
    "    # Find good balance between speed and accuracy\n",
    "    median_time = efficiency_df['fit_time'].median()\n",
    "    median_ll = efficiency_df['log_likelihood'].median()\n",
    "    \n",
    "    balanced_candidates = efficiency_df[\n",
    "        (efficiency_df['fit_time'] <= median_time * 1.5) &  # Not too slow\n",
    "        (efficiency_df['log_likelihood'] >= median_ll)      # Good performance\n",
    "    ]\n",
    "    \n",
    "    if len(balanced_candidates) > 0:\n",
    "        recommended_nsimul = balanced_candidates.loc[balanced_candidates['efficiency_ratio'].idxmax(), 'nSimul']\n",
    "        print(f\"   üåü Recommended for balanced performance: nSimul = {int(recommended_nsimul)}\")\n",
    "    \n",
    "    # Quick prototyping recommendation\n",
    "    fast_enough = efficiency_df[efficiency_df['fit_time'] <= efficiency_df['fit_time'].min() * 2]\n",
    "    if len(fast_enough) > 0:\n",
    "        proto_nsimul = fast_enough.loc[fast_enough['log_likelihood'].idxmax(), 'nSimul']\n",
    "        print(f\"   üöÄ Recommended for quick prototyping: nSimul = {int(proto_nsimul)}\")\n",
    "    \n",
    "    # Publication-quality recommendation\n",
    "    high_quality = efficiency_df[efficiency_df['log_likelihood'] >= efficiency_df['log_likelihood'].max() * 0.98]\n",
    "    if len(high_quality) > 0:\n",
    "        quality_nsimul = high_quality.loc[high_quality['fit_time'].idxmin(), 'nSimul']\n",
    "        print(f\"   üìö Recommended for publication quality: nSimul = {int(quality_nsimul)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Insufficient data for detailed recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407ed64",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Summary\n",
    "\n",
    "Create comprehensive plots showing parameter evolution, model fit quality, and computational efficiency across nSimul values, with recommendations for optimal settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('nSimul Optimization Analysis for Participant \"oy\"', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Log-Likelihood vs nSimul\n",
    "if len(performance_df) > 1:\n",
    "    axes[0, 0].plot(performance_df['nSimul'], performance_df['log_likelihood'], 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xlabel('nSimul')\n",
    "    axes[0, 0].set_ylabel('Log-Likelihood')\n",
    "    axes[0, 0].set_title('Model Fit Quality vs nSimul')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate best point\n",
    "    best_ll_idx = performance_df['log_likelihood'].idxmax()\n",
    "    best_nsimul = performance_df.loc[best_ll_idx, 'nSimul']\n",
    "    best_ll = performance_df.loc[best_ll_idx, 'log_likelihood']\n",
    "    axes[0, 0].annotate(f'Best: {int(best_nsimul)}', \n",
    "                       xy=(best_nsimul, best_ll), \n",
    "                       xytext=(10, 10), textcoords='offset points',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                       arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# 2. AIC vs nSimul\n",
    "if len(performance_df) > 1:\n",
    "    axes[0, 1].plot(performance_df['nSimul'], performance_df['AIC'], 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_xlabel('nSimul')\n",
    "    axes[0, 1].set_ylabel('AIC')\n",
    "    axes[0, 1].set_title('Model Selection Criterion vs nSimul')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate best point\n",
    "    best_aic_idx = performance_df['AIC'].idxmin()\n",
    "    best_aic_nsimul = performance_df.loc[best_aic_idx, 'nSimul']\n",
    "    best_aic = performance_df.loc[best_aic_idx, 'AIC']\n",
    "    axes[0, 1].annotate(f'Best: {int(best_aic_nsimul)}', \n",
    "                       xy=(best_aic_nsimul, best_aic), \n",
    "                       xytext=(10, 10), textcoords='offset points',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                       arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# 3. Fitting Time vs nSimul\n",
    "if len(efficiency_df) > 1:\n",
    "    axes[0, 2].plot(efficiency_df['nSimul'], efficiency_df['fit_time'], 'go-', linewidth=2, markersize=8)\n",
    "    axes[0, 2].set_xlabel('nSimul')\n",
    "    axes[0, 2].set_ylabel('Fit Time (seconds)')\n",
    "    axes[0, 2].set_title('Computational Cost vs nSimul')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Parameter Evolution\n",
    "if len(param_df) > 1:\n",
    "    key_params = ['lambda', 'sigma_av_a', 'sigma_av_v', 'p_c']\n",
    "    colors = ['blue', 'red', 'green', 'purple']\n",
    "    \n",
    "    for i, (param, color) in enumerate(zip(key_params, colors)):\n",
    "        axes[1, 0].plot(param_df['nSimul'], param_df[param], 'o-', \n",
    "                       color=color, label=param, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('nSimul')\n",
    "    axes[1, 0].set_ylabel('Parameter Value')\n",
    "    axes[1, 0].set_title('Parameter Stability vs nSimul')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Efficiency Analysis\n",
    "if len(efficiency_df) > 1:\n",
    "    axes[1, 1].plot(efficiency_df['nSimul'], efficiency_df['efficiency_ratio'], 'mo-', linewidth=2, markersize=8)\n",
    "    axes[1, 1].set_xlabel('nSimul')\n",
    "    axes[1, 1].set_ylabel('Efficiency (LL improvement / time)')\n",
    "    axes[1, 1].set_title('Cost-Benefit Efficiency vs nSimul')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate most efficient point\n",
    "    if efficiency_df['efficiency_ratio'].max() > 0:\n",
    "        best_eff_idx = efficiency_df['efficiency_ratio'].idxmax()\n",
    "        best_eff_nsimul = efficiency_df.loc[best_eff_idx, 'nSimul']\n",
    "        best_eff = efficiency_df.loc[best_eff_idx, 'efficiency_ratio']\n",
    "        axes[1, 1].annotate(f'Most efficient: {int(best_eff_nsimul)}', \n",
    "                           xy=(best_eff_nsimul, best_eff), \n",
    "                           xytext=(10, 10), textcoords='offset points',\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                           arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# 6. Summary Comparison\n",
    "if len(performance_df) > 1:\n",
    "    # Normalize all metrics to 0-1 scale for comparison\n",
    "    norm_performance = performance_df.copy()\n",
    "    \n",
    "    # Higher is better for LL, lower is better for AIC and time\n",
    "    norm_performance['norm_ll'] = (norm_performance['log_likelihood'] - norm_performance['log_likelihood'].min()) / \\\n",
    "                                 (norm_performance['log_likelihood'].max() - norm_performance['log_likelihood'].min())\n",
    "    \n",
    "    norm_performance['norm_aic'] = 1 - (norm_performance['AIC'] - norm_performance['AIC'].min()) / \\\n",
    "                                  (norm_performance['AIC'].max() - norm_performance['AIC'].min())\n",
    "    \n",
    "    norm_performance['norm_time'] = 1 - (norm_performance['fit_time'] - norm_performance['fit_time'].min()) / \\\n",
    "                                   (norm_performance['fit_time'].max() - norm_performance['fit_time'].min())\n",
    "    \n",
    "    # Combined score (equal weighting)\n",
    "    norm_performance['combined_score'] = (norm_performance['norm_ll'] + \n",
    "                                        norm_performance['norm_aic'] + \n",
    "                                        norm_performance['norm_time']) / 3\n",
    "    \n",
    "    # Plot normalized metrics\n",
    "    axes[1, 2].plot(norm_performance['nSimul'], norm_performance['norm_ll'], 'o-', label='LL (normalized)', linewidth=2)\n",
    "    axes[1, 2].plot(norm_performance['nSimul'], norm_performance['norm_aic'], 's-', label='AIC (normalized)', linewidth=2)\n",
    "    axes[1, 2].plot(norm_performance['nSimul'], norm_performance['norm_time'], '^-', label='Speed (normalized)', linewidth=2)\n",
    "    axes[1, 2].plot(norm_performance['nSimul'], norm_performance['combined_score'], 'd-', \n",
    "                   label='Combined Score', linewidth=3, markersize=8, color='black')\n",
    "    \n",
    "    axes[1, 2].set_xlabel('nSimul')\n",
    "    axes[1, 2].set_ylabel('Normalized Score (higher = better)')\n",
    "    axes[1, 2].set_title('Overall Performance Comparison')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate best combined score\n",
    "    best_combined_idx = norm_performance['combined_score'].idxmax()\n",
    "    best_combined_nsimul = norm_performance.loc[best_combined_idx, 'nSimul']\n",
    "    best_combined_score = norm_performance.loc[best_combined_idx, 'combined_score']\n",
    "    axes[1, 2].annotate(f'Optimal: {int(best_combined_nsimul)}', \n",
    "                       xy=(best_combined_nsimul, best_combined_score), \n",
    "                       xytext=(10, 10), textcoords='offset points',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='gold', alpha=0.8),\n",
    "                       arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        if not axes[i, j].has_data():\n",
    "            axes[i, j].text(0.5, 0.5, 'Insufficient Data\\nfor Analysis', \n",
    "                           ha='center', va='center', transform=axes[i, j].transAxes,\n",
    "                           fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary and recommendations\n",
    "print(\"üéØ FINAL SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(successful_fits) >= 2:\n",
    "    print(f\"‚úÖ Successfully tested {len(successful_fits)} nSimul values: {successful_fits}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    best_ll_nsimul = performance_df.loc[performance_df['log_likelihood'].idxmax(), 'nSimul']\n",
    "    best_aic_nsimul = performance_df.loc[performance_df['AIC'].idxmin(), 'nSimul']\n",
    "    fastest_nsimul = efficiency_df.loc[efficiency_df['fit_time'].idxmin(), 'nSimul']\n",
    "    \n",
    "    print(f\"\\nüìä KEY FINDINGS:\")\n",
    "    print(f\"   üèÜ Best log-likelihood: nSimul = {int(best_ll_nsimul)}\")\n",
    "    print(f\"   üèÜ Best AIC: nSimul = {int(best_aic_nsimul)}\")\n",
    "    print(f\"   ‚ö° Fastest fitting: nSimul = {int(fastest_nsimul)}\")\n",
    "    \n",
    "    if 'efficiency_ratio' in efficiency_df.columns and efficiency_df['efficiency_ratio'].max() > 0:\n",
    "        most_efficient_nsimul = efficiency_df.loc[efficiency_df['efficiency_ratio'].idxmax(), 'nSimul']\n",
    "        print(f\"   üí° Most efficient: nSimul = {int(most_efficient_nsimul)}\")\n",
    "    \n",
    "    # Parameter stability assessment\n",
    "    if len(param_df) > 1:\n",
    "        print(f\"\\nüéØ PARAMETER STABILITY:\")\n",
    "        key_params = ['lambda', 'sigma_av_a', 'sigma_av_v', 'p_c']\n",
    "        stable_params = []\n",
    "        unstable_params = []\n",
    "        \n",
    "        for param in key_params:\n",
    "            if param in param_df.columns:\n",
    "                cv = np.std(param_df[param]) / abs(np.mean(param_df[param])) * 100\n",
    "                if cv < 10:\n",
    "                    stable_params.append(f\"{param} (CV: {cv:.1f}%)\")\n",
    "                else:\n",
    "                    unstable_params.append(f\"{param} (CV: {cv:.1f}%)\")\n",
    "        \n",
    "        if stable_params:\n",
    "            print(f\"   ‚úÖ Stable parameters: {', '.join(stable_params)}\")\n",
    "        if unstable_params:\n",
    "            print(f\"   ‚ö†Ô∏è  Unstable parameters: {', '.join(unstable_params)}\")\n",
    "    \n",
    "    # Overfitting assessment\n",
    "    print(f\"\\nüîç OVERFITTING ASSESSMENT:\")\n",
    "    \n",
    "    # Check if higher nSimul consistently improves performance\n",
    "    sorted_perf = performance_df.sort_values('nSimul')\n",
    "    ll_trend = np.corrcoef(sorted_perf['nSimul'], sorted_perf['log_likelihood'])[0, 1]\n",
    "    \n",
    "    if ll_trend > 0.5:\n",
    "        print(f\"   ‚úÖ No clear overfitting - LL improves with nSimul (r={ll_trend:.2f})\")\n",
    "    elif ll_trend < -0.5:\n",
    "        print(f\"   ‚ùå Possible overfitting - LL decreases with nSimul (r={ll_trend:.2f})\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Unclear trend - mixed results (r={ll_trend:.2f})\")\n",
    "    \n",
    "    # Computational efficiency\n",
    "    print(f\"\\n‚è±Ô∏è  COMPUTATIONAL EFFICIENCY:\")\n",
    "    \n",
    "    time_range = efficiency_df['fit_time'].max() - efficiency_df['fit_time'].min()\n",
    "    ll_range = performance_df['log_likelihood'].max() - performance_df['log_likelihood'].min()\n",
    "    \n",
    "    if time_range > 0:\n",
    "        time_per_ll_improvement = time_range / ll_range if ll_range > 0 else float('inf')\n",
    "        print(f\"   üìä Time cost per LL improvement: {time_per_ll_improvement:.1f} seconds\")\n",
    "        \n",
    "        if time_per_ll_improvement < 10:\n",
    "            print(f\"   ‚úÖ Good efficiency - modest time cost for improvements\")\n",
    "        elif time_per_ll_improvement < 60:\n",
    "            print(f\"   ‚ö†Ô∏è  Moderate efficiency - consider time vs. accuracy trade-off\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Poor efficiency - high time cost for small improvements\")\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
    "    \n",
    "    # Calculate a balanced recommendation\n",
    "    if len(performance_df) > 1:\n",
    "        # Normalize and combine scores\n",
    "        norm_ll = (performance_df['log_likelihood'] - performance_df['log_likelihood'].min()) / \\\n",
    "                 (performance_df['log_likelihood'].max() - performance_df['log_likelihood'].min())\n",
    "        norm_time = 1 - (efficiency_df['fit_time'] - efficiency_df['fit_time'].min()) / \\\n",
    "                   (efficiency_df['fit_time'].max() - efficiency_df['fit_time'].min())\n",
    "        \n",
    "        # Weighted score: 60% performance, 40% speed\n",
    "        balanced_score = 0.6 * norm_ll + 0.4 * norm_time\n",
    "        recommended_idx = balanced_score.idxmax()\n",
    "        recommended_nsimul = performance_df.loc[recommended_idx, 'nSimul']\n",
    "        \n",
    "        print(f\"   üåü RECOMMENDED nSimul = {int(recommended_nsimul)}\")\n",
    "        print(f\"      (Optimal balance of accuracy and computational efficiency)\")\n",
    "        \n",
    "        # Context-specific recommendations\n",
    "        print(f\"\\n   üìã CONTEXT-SPECIFIC RECOMMENDATIONS:\")\n",
    "        print(f\"      üöÄ For quick prototyping: nSimul = {int(fastest_nsimul)}\")\n",
    "        print(f\"      üìä For best model fit: nSimul = {int(best_ll_nsimul)}\")\n",
    "        print(f\"      üèÅ For model comparison: nSimul = {int(best_aic_nsimul)}\")\n",
    "        \n",
    "        # Time-based recommendations\n",
    "        fast_options = efficiency_df[efficiency_df['fit_time'] <= 30]  # Under 30 seconds\n",
    "        if len(fast_options) > 0:\n",
    "            fast_recommendation = fast_options.loc[fast_options['log_likelihood'].idxmax(), 'nSimul'] \\\n",
    "                                 if 'log_likelihood' in performance_df.columns else fast_options['nSimul'].iloc[0]\n",
    "            print(f\"      ‚ö° For time-constrained analysis: nSimul = {int(fast_recommendation)}\")\n",
    "    \n",
    "    # Warnings and caveats\n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT CAVEATS:\")\n",
    "    print(f\"   ‚Ä¢ Results based on single participant ('oy') - may vary across participants\")\n",
    "    print(f\"   ‚Ä¢ Monte Carlo variance introduces randomness - results may vary between runs\")\n",
    "    print(f\"   ‚Ä¢ Optimal nSimul may depend on model complexity and data characteristics\")\n",
    "    print(f\"   ‚Ä¢ Consider computational resources and time constraints in your specific context\")\n",
    "    \n",
    "    # Future testing suggestions\n",
    "    print(f\"\\nüîÆ SUGGESTED FOLLOW-UP TESTS:\")\n",
    "    print(f\"   ‚Ä¢ Test optimal nSimul on additional participants\")\n",
    "    print(f\"   ‚Ä¢ Verify recommendations with different model configurations\")\n",
    "    print(f\"   ‚Ä¢ Test stability with multiple random seeds\")\n",
    "    print(f\"   ‚Ä¢ Consider adaptive nSimul based on convergence criteria\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Insufficient successful fits ({len(successful_fits)}) for comprehensive analysis\")\n",
    "    print(f\"   Consider:\")\n",
    "    print(f\"   ‚Ä¢ Checking data quality and format\")\n",
    "    print(f\"   ‚Ä¢ Adjusting model configuration\")\n",
    "    print(f\"   ‚Ä¢ Using simpler parameter bounds\")\n",
    "    print(f\"   ‚Ä¢ Increasing optimization attempts (nStart)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"üéâ nSimul optimization analysis complete!\")\n",
    "print(f\"üí° Use these recommendations to optimize your Monte Carlo fitting pipeline.\")\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab4da4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive analysis of nSimul optimization for Monte Carlo causal inference fitting. Key outputs include:\n",
    "\n",
    "### üéØ Main Results\n",
    "- **Performance Comparison**: Log-likelihood, AIC, and BIC across different nSimul values\n",
    "- **Parameter Stability**: How fitted parameters change with simulation count\n",
    "- **Computational Efficiency**: Time vs. accuracy trade-offs\n",
    "- **Overfitting Detection**: Stability analysis and variance assessment\n",
    "\n",
    "### üìä Key Metrics Analyzed\n",
    "1. **Model Fit Quality**: Log-likelihood improvement with higher nSimul\n",
    "2. **Selection Criteria**: AIC/BIC for model comparison\n",
    "3. **Parameter Convergence**: Stability and consistency of fitted parameters\n",
    "4. **Computational Cost**: Fitting time scaling with nSimul\n",
    "5. **Efficiency Ratios**: Performance improvement per unit time\n",
    "\n",
    "### üéØ Practical Outcomes\n",
    "- **Recommended nSimul**: Optimal value balancing accuracy and speed\n",
    "- **Context-Specific Recommendations**: Different values for different use cases\n",
    "- **Overfitting Assessment**: Whether higher nSimul improves or hurts performance\n",
    "- **Resource Planning**: Time and memory requirements for different settings\n",
    "\n",
    "### üîÆ Next Steps\n",
    "1. Apply optimal nSimul to full participant dataset\n",
    "2. Validate recommendations across different model configurations\n",
    "3. Test stability with different random seeds\n",
    "4. Consider adaptive nSimul strategies based on convergence\n",
    "\n",
    "Use the recommended nSimul values to optimize your Monte Carlo fitting pipeline for the best balance of accuracy, stability, and computational efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
