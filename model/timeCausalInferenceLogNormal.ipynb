{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce061eb8",
   "metadata": {},
   "source": [
    "## Log-Normal Distribution for Duration Estimation\n",
    "\n",
    "For duration perception, using a log-normal distribution is much more appropriate because:\n",
    "\n",
    "1. **Positivity**: Both stimulus durations and observations are naturally positive\n",
    "2. **Weber's Law**: Noise scales proportionally with duration magnitude\n",
    "3. **Perceptual Scale**: Duration discrimination follows logarithmic scaling\n",
    "\n",
    "### Mathematical Framework:\n",
    "- Let $\\log(S)$ be the log-transformed true duration\n",
    "- Measurements follow: $\\log(m) \\sim \\mathcal{N}(\\log(S), \\sigma_{log}^2)$\n",
    "- This means: $m \\sim \\text{LogNormal}(\\log(S), \\sigma_{log}^2)$\n",
    "\n",
    "### Implementation Strategy:\n",
    "1. Transform durations to log space: $\\log(S)$, $\\log(m)$\n",
    "2. Perform all computations in log space (Gaussian operations)\n",
    "3. Transform back to linear space when needed: $\\exp(\\log(S))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Normal Duration Estimation Implementation\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ed8b60769b4aa0afdfb66242887635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.2, description='σ_log (noise in log space)', max=0.8, min=0.05, step…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison: Gaussian vs Log-Normal\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f22d1a3dbb46ae944f9c2233cafdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.3, description='σ_log', max=0.6, min=0.1, step=0.05), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compareDistributions(sigma_log, S)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import lognorm, norm\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Log-normal implementation for duration estimation\n",
    "def unimodalMeasurementsLogNormal(sigma_log, S, n=500):\n",
    "    \"\"\"\n",
    "    Generate measurements using log-normal distribution\n",
    "    Args:\n",
    "        sigma_log: standard deviation in log space\n",
    "        S: true duration (in linear space)\n",
    "        n: number of measurements\n",
    "    Returns:\n",
    "        measurements: array of measurements following log-normal distribution\n",
    "    \"\"\"\n",
    "    # Generate log-normal measurements\n",
    "    # lognorm.rvs(s=sigma_log, scale=S) where s is the shape parameter (sigma in log space)\n",
    "    measurements = lognorm.rvs(s=sigma_log, scale=S, size=n)\n",
    "    return measurements\n",
    "\n",
    "def logNormalPDF(x, S, sigma_log):\n",
    "    \"\"\"\n",
    "    Probability density function for log-normal distribution\n",
    "    Args:\n",
    "        x: measurement values\n",
    "        S: true duration (scale parameterq)\n",
    "        sigma_log: standard deviation in log space\n",
    "    \"\"\"\n",
    "    return lognorm.pdf(x, s=sigma_log, scale=S)\n",
    "\n",
    "def likelihoodLogNormal(S, sigma_log):\n",
    "    \"\"\"\n",
    "    Likelihood function for log-normal measurements\n",
    "    Args:\n",
    "        S: true duration\n",
    "        sigma_log: standard deviation in log space\n",
    "    \"\"\"\n",
    "    # Create range from very small positive value to reasonable upper bound\n",
    "    m = np.linspace(0.01, S + 5*S*sigma_log, 500)\n",
    "    p_m = logNormalPDF(m, S, sigma_log)\n",
    "    return m, p_m\n",
    "\n",
    "def plotLikelihoodLogNormal(S, sigma_log):\n",
    "    \"\"\"Plot likelihood function for log-normal distribution\"\"\"\n",
    "    x, p_x = likelihoodLogNormal(S, sigma_log)\n",
    "    plt.plot(x, p_x, label='Log-Normal Likelihood', linewidth=2)\n",
    "    plt.xlabel('Measurement Values')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title('Log-Normal Measurement Distribution $p(m|S)$')\n",
    "    plt.legend()\n",
    "\n",
    "def plotMeasurementsLogNormal(sigma_log, S):\n",
    "    \"\"\"Plot histogram of log-normal measurements\"\"\"\n",
    "    m = unimodalMeasurementsLogNormal(sigma_log, S)\n",
    "    plt.hist(m, bins=50, density=True, alpha=0.5, label='Measurements Histogram')\n",
    "    plt.xlabel('Measurement Values')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Log-Normal Measurements Histogram')\n",
    "    plt.legend()\n",
    "\n",
    "def plotMeasurementsAndLikelihoodLogNormal(sigma_log, S):\n",
    "    \"\"\"Compare measurements histogram with theoretical likelihood\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot measurements\n",
    "    plotMeasurementsLogNormal(sigma_log, S)\n",
    "    \n",
    "    # Plot likelihood\n",
    "    plotLikelihoodLogNormal(S, sigma_log)\n",
    "    \n",
    "    # Add vertical line for true stimulus\n",
    "    plt.axvline(S, color='red', linestyle='--', label=f'True Duration S={S:.2f}')\n",
    "    \n",
    "    # Add mean of log-normal distribution\n",
    "    mean_lognormal = S * np.exp(0.5 * sigma_log**2)\n",
    "    mean_lognormal_pdf = np.mean(unimodalMeasurementsLogNormal(sigma_log, S))\n",
    "    plt.axvline(mean_lognormal, color='orange', linestyle='--', \n",
    "                label=f'Expected Value={mean_lognormal:.2f}')\n",
    "    plt.axvline(mean_lognormal_pdf, color='purple', linestyle='--', \n",
    "                label=f'Mean PDF={mean_lognormal_pdf:.2f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, max(S + 3*S*sigma_log, 2))\n",
    "    plt.show()\n",
    "\n",
    "# Interactive comparison between Gaussian and Log-Normal\n",
    "def compareDistributions(sigma_log, S):\n",
    "    \"\"\"Compare Gaussian vs Log-Normal distributions for duration estimation\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Gaussian (original approach)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    # For Gaussian, use sigma_log as the linear sigma (though this isn't ideal)\n",
    "    m_gauss = np.random.normal(S, sigma_log * S, 1000)  # Scale sigma with S\n",
    "    # Remove negative values (hack for Gaussian)\n",
    "    m_gauss = m_gauss[m_gauss > 0]\n",
    "    plt.hist(m_gauss, bins=30, density=True, alpha=0.7, color='blue', label='Gaussian')\n",
    "    plt.axvline(S, color='red', linestyle='--', label=f'True S={S:.2f}')\n",
    "    plt.title('Gaussian Distribution\\n(with negative values removed)')\n",
    "    plt.xlabel('Measurement')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log-Normal (proposed approach)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    m_lognorm = unimodalMeasurementsLogNormal(sigma_log, S, 1000)\n",
    "    plt.hist(m_lognorm, bins=30, density=True, alpha=0.7, color='green', label='Log-Normal')\n",
    "    plt.axvline(S, color='red', linestyle='--', label=f'True S={S:.2f}')\n",
    "    mean_lognormal = S * np.exp(0.5 * sigma_log**2)\n",
    "    plt.axvline(mean_lognormal, color='orange', linestyle='--', \n",
    "                label=f'E[m]={mean_lognormal:.2f}')\n",
    "    plt.title('Log-Normal Distribution\\n(naturally positive)')\n",
    "    plt.xlabel('Measurement')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weber's Law demonstration\n",
    "    plt.subplot(1, 3, 3)\n",
    "    S_values = np.linspace(0.1, 2.0, 10)\n",
    "    cv_lognormal = []  # Coefficient of variation\n",
    "    \n",
    "    for S_val in S_values:\n",
    "        measurements = unimodalMeasurementsLogNormal(sigma_log, S_val, 1000)\n",
    "        cv = np.std(measurements) / np.mean(measurements)\n",
    "        cv_lognormal.append(cv)\n",
    "    \n",
    "    plt.plot(S_values, cv_lognormal, 'o-', color='green', label='Log-Normal CV')\n",
    "    plt.axhline(sigma_log, color='red', linestyle='--', \n",
    "                label=f'σ_log = {sigma_log:.2f}')\n",
    "    plt.title(\"Weber's Law: CV vs Duration\")\n",
    "    plt.xlabel('True Duration S')\n",
    "    plt.ylabel('Coefficient of Variation')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets\n",
    "print(\"Log-Normal Duration Estimation Implementation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "interact(plotMeasurementsAndLikelihoodLogNormal,\n",
    "         sigma_log=FloatSlider(value=0.2, min=0.05, max=0.8, step=0.05, \n",
    "                               description='σ_log (noise in log space)'),\n",
    "         S=FloatSlider(value=0.5, min=0.1, max=2.0, step=0.1, \n",
    "                       description='True Duration S'))\n",
    "\n",
    "print(\"\\nComparison: Gaussian vs Log-Normal\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "interact(compareDistributions,\n",
    "         sigma_log=FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05, \n",
    "                               description='σ_log'),\n",
    "         S=FloatSlider(value=0.8, min=0.2, max=2.0, step=0.2, \n",
    "                       description='True Duration S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a52d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2f8f6ca21d41dea6d88902ca52a235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='True Duration S', max=2.0, min=0.1), FloatSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_lognormal_perspectives(true_S=0.5, sigma_log=0.4, n_measurements=5)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "def plot_lognormal_perspectives(true_S=0.5, sigma_log=0.4, n_measurements=5):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # For lognormal, we need to convert parameters\n",
    "    # If X is lognormal, then log(X) is normal with mean mu and std sigma_log\n",
    "    mu_log = np.log(true_S) - sigma_log**2/2  # Adjustment to make the mean of lognormal = true_S\n",
    "    \n",
    "    # 1. Experimenter's perspective: Measurement distribution p(m|s)\n",
    "    m_range = np.linspace(0.01, true_S*3, 1000)  # Avoid m=0 for lognormal\n",
    "    measurement_dist = stats.lognorm.pdf(m_range, sigma_log, scale=np.exp(mu_log))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(m_range, measurement_dist, 'b-', linewidth=2, \n",
    "             label=f'Measurement Distribution p(m|s={true_S})')\n",
    "    plt.fill_between(m_range, measurement_dist, alpha=0.2, color='blue')\n",
    "    plt.xlabel('Possible measurements (m)')\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.title(\"Experimenter's Perspective: Lognormal Measurement Distribution\")\n",
    "    \n",
    "    # Generate some example measurements\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    measurements = np.random.lognormal(mean=mu_log, sigma=sigma_log, size=n_measurements)\n",
    "    \n",
    "    # Add measurements as vertical lines\n",
    "    for i, m in enumerate(measurements):\n",
    "        plt.axvline(m, color=f'C{i+1}', linestyle='--', \n",
    "                    label=f'Measurement {i+1}: m={m:.2f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Observer's perspective: Likelihood functions L(s|m)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    s_range = np.linspace(0.01, true_S*3, 1000)  # Avoid s=0 for lognormal\n",
    "    \n",
    "    # Plot likelihood functions for each measurement\n",
    "    for i, m in enumerate(measurements):\n",
    "        likelihoods = []\n",
    "        for s in s_range:\n",
    "            # For each possible stimulus value s, calculate mu_log for that s\n",
    "            s_mu_log = np.log(s) - sigma_log**2/2\n",
    "            # Likelihood is proportional to p(m|s)\n",
    "            likelihood = stats.lognorm.pdf(m, sigma_log, scale=np.exp(s_mu_log))\n",
    "\n",
    "            likelihoods.append(likelihood)\n",
    "        \n",
    "        # Normalize for better visualization\n",
    "        likelihoods = np.array(likelihoods) / np.max(likelihoods)\n",
    "        \n",
    "        plt.plot(s_range, likelihoods, color=f'C{i+1}', \n",
    "                 label=f'Likelihood L(s|m={m:.2f})')\n",
    "        plt.axvline(m, color=f'C{i+1}', linestyle='--', \n",
    "                    label=f'Measurement {i+1}: m={m:.2f}')\n",
    "\n",
    "    #print(f\"Likelihood for measurement {i+1} (m={m:.2f}): {likelihoods[:100]}...\")\n",
    "\n",
    "    plt.axvline(true_S, color='k', linestyle='-', label='True stimulus')\n",
    "    plt.xlabel('Possible stimulus values (s)')\n",
    "    plt.ylabel('Likelihood (normalized)')\n",
    "    plt.title(\"Observer's Perspective: Likelihood Functions\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_lognormal_perspectives,\n",
    "         true_S=FloatSlider(value=0.5, min=0.1, max=2.0, step=0.1, \n",
    "                            description='True Duration S'),\n",
    "         sigma_log=FloatSlider(value=0.4, min=0.1, max=1.0, step=0.1, \n",
    "                               description='σ_log (noise in log space)'),\n",
    "         n_measurements=widgets.IntSlider(value=1, min=1, max=10, step=1, \n",
    "                                          description='Number of Measurements'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fbf444",
   "metadata": {},
   "source": [
    "## Important Distinction: Experimenter vs Observer Perspectives\n",
    "\n",
    "### **Experimenter's Perspective**: Measurement Distribution p(m|s)\n",
    "- **What it shows**: How measurements are distributed when the experimenter presents a specific stimulus S\n",
    "- **The process**: Stimulus S → Perceptual system (with noise) → Measurement m\n",
    "- **Distribution**: m ~ LogNormal(log(S), σ_log²) \n",
    "- **Key insight**: The log-normal distribution describes the **measurement noise**, not the stimulus generation\n",
    "\n",
    "### **Observer's Perspective**: Likelihood Function L(s|m)  \n",
    "- **What it shows**: Given a measurement m, how likely is each possible stimulus value s?\n",
    "- **The process**: Observed measurement m → Infer possible stimulus values s\n",
    "- **Mathematical relationship**: L(s|m) ∝ p(m|s)\n",
    "- **Key insight**: This is Bayesian inference - using the measurement to estimate the stimulus\n",
    "\n",
    "### **Why Log-Normal for Measurements?**\n",
    "1. **Weber's Law**: Perceptual noise scales with stimulus magnitude\n",
    "2. **Positivity**: Both stimuli and measurements must be positive (durations > 0)\n",
    "3. **Empirical evidence**: Duration perception shows multiplicative noise\n",
    "\n",
    "### **Correct Interpretation**:\n",
    "- **NOT**: \"Stimuli are log-normally distributed in the experiment\"\n",
    "- **YES**: \"Measurements of stimuli are log-normally distributed due to perceptual noise\"\n",
    "\n",
    "The experimenter presents discrete stimulus values, but the observer's perceptual system adds log-normal noise to create the measurement distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0551270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Causal Inference with Log-Normal Distributions\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02eb46f9d97c41d691757d33c9d6a7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.3, description='σ_A (log)', max=0.6, min=0.1, step=0.05), FloatSlide…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotCausalInferenceResultsLogNormal(sigma_A_log=0.3, sigma_V_log=0.25, S_true=0.8, pC_prior=0.7)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Causal Inference with Log-Normal Distributions\n",
    "\n",
    "def bimodalMeasurementsLogNormal(sigma_A_log, sigma_V_log, S, n=500):\n",
    "    \"\"\"\n",
    "    Generate bimodal measurements using log-normal distributions\n",
    "    Args:\n",
    "        sigma_A_log: auditory noise in log space\n",
    "        sigma_V_log: visual noise in log space\n",
    "        S: true duration\n",
    "        n: number of measurements\n",
    "    \"\"\"\n",
    "    # Auditory measurements\n",
    "    mA = lognorm.rvs(s=sigma_A_log, scale=S, size=n)\n",
    "    \n",
    "    # Visual measurements\n",
    "    mV = lognorm.rvs(s=sigma_V_log, scale=S, size=n)\n",
    "    \n",
    "    return mA, mV\n",
    "\n",
    "def causalInferenceLogNormal(mA, mV, sigma_A_log, sigma_V_log, pC):\n",
    "    \"\"\"\n",
    "    Causal inference with log-normal measurements\n",
    "    Working in log space for computations\n",
    "    \"\"\"\n",
    "    # Convert measurements to log space\n",
    "    log_mA = np.log(mA)\n",
    "    log_mV = np.log(mV)\n",
    "    \n",
    "    # Prior parameters in log space (assume log(S) ~ N(mu_prior, sigma_prior^2))\n",
    "    mu_prior_log = 0  # Assuming geometric mean of 1 (log(1) = 0)\n",
    "    sigma_prior_log = 1.0  # Prior uncertainty in log space\n",
    "    \n",
    "    # Compute posterior for C=1 (common cause) in log space\n",
    "    # Combined measurement: weighted average in log space\n",
    "    J_a = 1 / sigma_A_log**2\n",
    "    J_v = 1 / sigma_V_log**2\n",
    "    J_p = 1 / sigma_prior_log**2\n",
    "    \n",
    "    # Posterior precision and mean for common cause (in log space)\n",
    "    precision_C1 = J_a + J_v + J_p\n",
    "    mu_C1_log = (J_a * log_mA + J_v * log_mV + \n",
    "                 J_p * mu_prior_log) / precision_C1\n",
    "    sigma_C1_log = 1 / np.sqrt(precision_C1)\n",
    "    \n",
    "    # Posterior for C=0 (separate causes) in log space\n",
    "    # Auditory estimate\n",
    "    J_a_post = J_a + J_p\n",
    "    mu_A_post_log = (J_a * log_mA + J_p * mu_prior_log) / J_a_post\n",
    "    sigma_A_post_log = 1 / np.sqrt(J_a_post)\n",
    "    \n",
    "    # Visual estimate  \n",
    "    J_v_post = J_v + J_p\n",
    "    mu_V_post_log = (J_v * log_mV + J_p * mu_prior_log) / J_v_post\n",
    "    sigma_V_post_log = 1 / np.sqrt(J_v_post)\n",
    "    \n",
    "    # Likelihood of measurements under each causal structure\n",
    "    # P(mA, mV | C=1): joint likelihood in log space\n",
    "    diff_log = log_mA - log_mV\n",
    "    sigma_diff_log = np.sqrt(sigma_A_log**2 + sigma_V_log**2)\n",
    "    likelihood_C1 = norm.pdf(diff_log, 0, sigma_diff_log)\n",
    "    \n",
    "    # P(mA, mV | C=0): independent likelihoods\n",
    "    likelihood_C0 = 1  # Normalized constant (doesn't affect relative probabilities)\n",
    "    \n",
    "    # Posterior probability of common cause\n",
    "    evidence_C1 = likelihood_C1 * pC\n",
    "    evidence_C0 = likelihood_C0 * (1 - pC)\n",
    "    pC_post = evidence_C1 / (evidence_C1 + evidence_C0)\n",
    "    \n",
    "    # Final estimates (convert back to linear space)\n",
    "    if pC_post > 0.5:\n",
    "        # Common cause: use integrated estimate\n",
    "        S_hat = np.exp(mu_C1_log)\n",
    "        causal_structure = 'Common'\n",
    "    else:\n",
    "        # Separate causes: use modality-specific estimates\n",
    "        S_hat_A = np.exp(mu_A_post_log)\n",
    "        S_hat_V = np.exp(mu_V_post_log)\n",
    "        # For demonstration, return auditory estimate\n",
    "        S_hat = S_hat_A\n",
    "        causal_structure = 'Separate'\n",
    "    \n",
    "    return S_hat, pC_post, causal_structure\n",
    "\n",
    "def simulateCausalInferenceLogNormal(sigma_A_log=0.3, sigma_V_log=0.25, \n",
    "                                   S_true=0.8, pC_prior=0.7, n_trials=1000):\n",
    "    \"\"\"\n",
    "    Simulate causal inference experiment with log-normal measurements\n",
    "    \"\"\"\n",
    "    # Generate measurements\n",
    "    mA, mV = bimodalMeasurementsLogNormal(sigma_A_log, sigma_V_log, S_true, n_trials)\n",
    "    \n",
    "    # Perform causal inference for each trial\n",
    "    S_estimates = []\n",
    "    pC_posteriors = []\n",
    "    causal_decisions = []\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        S_hat, pC_post, decision = causalInferenceLogNormal(\n",
    "            mA[i], mV[i], sigma_A_log, sigma_V_log, pC_prior)\n",
    "        \n",
    "        S_estimates.append(S_hat)\n",
    "        pC_posteriors.append(pC_post)\n",
    "        causal_decisions.append(decision)\n",
    "    \n",
    "    return np.array(S_estimates), np.array(pC_posteriors), causal_decisions, mA, mV\n",
    "\n",
    "def plotCausalInferenceResultsLogNormal(sigma_A_log=0.3, sigma_V_log=0.25, \n",
    "                                       S_true=0.8, pC_prior=0.7):\n",
    "    \"\"\"\n",
    "    Plot results of causal inference simulation with log-normal distributions\n",
    "    \"\"\"\n",
    "    S_estimates, pC_posteriors, decisions, mA, mV = simulateCausalInferenceLogNormal(\n",
    "        sigma_A_log, sigma_V_log, S_true, pC_prior, 1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Raw measurements\n",
    "    axes[0, 0].scatter(mA[:100], mV[:100], alpha=0.6, s=20)\n",
    "    axes[0, 0].plot([0, max(max(mA), max(mV))], [0, max(max(mA), max(mV))], \n",
    "                    'r--', alpha=0.7, label='Unity line')\n",
    "    axes[0, 0].axvline(S_true, color='red', linestyle=':', label=f'True S={S_true}')\n",
    "    axes[0, 0].axhline(S_true, color='red', linestyle=':', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Auditory Measurement')\n",
    "    axes[0, 0].set_ylabel('Visual Measurement')\n",
    "    axes[0, 0].set_title('Raw Measurements (Log-Normal)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Posterior probabilities\n",
    "    axes[0, 1].hist(pC_posteriors, bins=50, alpha=0.7, density=True)\n",
    "    axes[0, 1].axvline(pC_prior, color='red', linestyle='--', \n",
    "                       label=f'Prior p(C=1)={pC_prior}')\n",
    "    axes[0, 1].axvline(np.mean(pC_posteriors), color='orange', linestyle='--',\n",
    "                       label=f'Mean Posterior={np.mean(pC_posteriors):.3f}')\n",
    "    axes[0, 1].set_xlabel('Posterior p(C=1|mA,mV)')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].set_title('Causal Inference Posteriors')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Duration estimates\n",
    "    axes[0, 2].hist(S_estimates, bins=50, alpha=0.7, density=True, \n",
    "                    label='Estimates')\n",
    "    axes[0, 2].axvline(S_true, color='red', linestyle='--', \n",
    "                       label=f'True Duration={S_true}')\n",
    "    axes[0, 2].axvline(np.mean(S_estimates), color='orange', linestyle='--',\n",
    "                       label=f'Mean Estimate={np.mean(S_estimates):.3f}')\n",
    "    axes[0, 2].set_xlabel('Duration Estimate')\n",
    "    axes[0, 2].set_ylabel('Density')\n",
    "    axes[0, 2].set_title('Final Duration Estimates')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Causal decisions over time\n",
    "    common_indices = [i for i, d in enumerate(decisions) if d == 'Common']\n",
    "    separate_indices = [i for i, d in enumerate(decisions) if d == 'Separate']\n",
    "    \n",
    "    axes[1, 0].scatter(common_indices[:50], [S_estimates[i] for i in common_indices[:50]], \n",
    "                       alpha=0.7, color='blue', label='Common Cause', s=30)\n",
    "    axes[1, 0].scatter(separate_indices[:50], [S_estimates[i] for i in separate_indices[:50]], \n",
    "                       alpha=0.7, color='red', label='Separate Causes', s=30)\n",
    "    axes[1, 0].axhline(S_true, color='black', linestyle='--', alpha=0.7, \n",
    "                       label=f'True Duration={S_true}')\n",
    "    axes[1, 0].set_xlabel('Trial Number')\n",
    "    axes[1, 0].set_ylabel('Duration Estimate')\n",
    "    axes[1, 0].set_title('Estimates by Causal Decision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = S_estimates - S_true\n",
    "    axes[1, 1].hist(errors, bins=50, alpha=0.7, density=True)\n",
    "    axes[1, 1].axvline(0, color='red', linestyle='--', label='No error')\n",
    "    axes[1, 1].axvline(np.mean(errors), color='orange', linestyle='--',\n",
    "                       label=f'Mean Error={np.mean(errors):.3f}')\n",
    "    axes[1, 1].set_xlabel('Estimation Error')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Error Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weber's Law analysis\n",
    "    S_values = np.linspace(0.2, 2.0, 10)\n",
    "    cv_values = []\n",
    "    \n",
    "    for S_val in S_values:\n",
    "        estimates_temp, _, _, _, _ = simulateCausalInferenceLogNormal(\n",
    "            sigma_A_log, sigma_V_log, S_val, pC_prior, 200)\n",
    "        cv = np.std(estimates_temp) / np.mean(estimates_temp)\n",
    "        cv_values.append(cv)\n",
    "    \n",
    "    axes[1, 2].plot(S_values, cv_values, 'o-', color='green', linewidth=2)\n",
    "    axes[1, 2].set_xlabel('True Duration')\n",
    "    axes[1, 2].set_ylabel('Coefficient of Variation')\n",
    "    axes[1, 2].set_title(\"Weber's Law: CV vs Duration\")\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Summary Statistics (Log-Normal Model):\")\n",
    "    print(f\"True Duration: {S_true:.3f}\")\n",
    "    print(f\"Mean Estimate: {np.mean(S_estimates):.3f} ± {np.std(S_estimates):.3f}\")\n",
    "    print(f\"Bias: {np.mean(errors):.3f}\")\n",
    "    print(f\"RMSE: {np.sqrt(np.mean(errors**2)):.3f}\")\n",
    "    print(f\"Proportion Common Cause Decisions: {len(common_indices)/len(decisions):.3f}\")\n",
    "\n",
    "# Interactive exploration\n",
    "print(\"\\nCausal Inference with Log-Normal Distributions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "interact(plotCausalInferenceResultsLogNormal,\n",
    "         sigma_A_log=FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05, \n",
    "                                description='σ_A (log)'),\n",
    "         sigma_V_log=FloatSlider(value=0.25, min=0.1, max=0.6, step=0.05, \n",
    "                                description='σ_V (log)'),\n",
    "         S_true=FloatSlider(value=0.8, min=0.2, max=2.0, step=0.1, \n",
    "                           description='True Duration'),\n",
    "         pC_prior=FloatSlider(value=0.7, min=0.1, max=0.9, step=0.1, \n",
    "                             description='Prior p(C=1)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5ba20",
   "metadata": {},
   "source": [
    "## Key Advantages of Log-Normal Approach\n",
    "\n",
    "### 1. **Theoretical Justification**\n",
    "- **Weber's Law**: Discrimination thresholds scale with stimulus magnitude\n",
    "- **Natural Positivity**: No need for ad-hoc constraints to prevent negative durations\n",
    "- **Perceptual Scaling**: Matches logarithmic nature of duration perception\n",
    "\n",
    "### 2. **Mathematical Benefits**\n",
    "- **Gaussian Operations in Log Space**: All computations use familiar Gaussian math\n",
    "- **Multiplicative Noise**: More realistic for duration perception\n",
    "- **Stable Parameter Estimation**: Better numerical properties\n",
    "\n",
    "### 3. **Empirical Support**\n",
    "- Duration discrimination follows Weber's law\n",
    "- Reaction time distributions are typically log-normal\n",
    "- Neural timing mechanisms suggest multiplicative noise\n",
    "\n",
    "### 4. **Implementation Advantages**\n",
    "- **Clean Bayesian Updates**: Standard Gaussian inference in log space\n",
    "- **No Boundary Issues**: Natural positivity constraint\n",
    "- **Scalable Noise**: Automatically handles magnitude-dependent uncertainty\n",
    "\n",
    "### Key Transformation:\n",
    "```\n",
    "Original: m ~ N(S, σ²)        # Can be negative, constant absolute noise\n",
    "Log-Normal: log(m) ~ N(log(S), σ_log²)  # Always positive, relative noise\n",
    "```\n",
    "\n",
    "This approach naturally implements Weber's law where noise scales with duration magnitude!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
