{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4293cb8",
   "metadata": {},
   "source": [
    "# Simulation of Audio-Visual Duration Estimation with Causal Inference Model\n",
    "This notebook simulates the audio-visual duration estimation task using a causal inference model. It allows you to explore how different parameters affect the estimates through interactive controls.\n",
    "### Experimenter knows the true durations:\n",
    "Standard duration($S_s$) is always 0.5s, test duration varies($S_t$).\n",
    "We simulate and give the parameters to the model, and see how the estimates change.\n",
    "Parameters:\n",
    "- $\\sigma_a$: Standard deviation of auditory measurement noise\n",
    "- $\\sigma_v$: Standard deviation of visual measurement noise\n",
    "- $p_c$: Prior probability of common cause\n",
    "### Parameters we dont need to adjust specifically:\n",
    "- $c$: Conflict level between auditory and visual stimuli\n",
    "- $tmin$ and $tmax$: Minimum and maximum test durations\n",
    "\n",
    "### Steps:\n",
    "1. Create and repeated array of standard durations: S_s: [0.5, 0.5, ..., 0.5]\n",
    "2. Create an array of duration differences: delta: [0.0, 0.1, ..., 2.0]\n",
    "3. Create constant conflict durations: c:[-250,-167,-83,0,83,167,250]ms\n",
    "4. For each combination of S_s, delta, and c:\n",
    "    - Calculate test duration: S_t = S_s + delta + c\n",
    "5. For simplicity we dont need to estimate noisy measurements, we can directly use the true durations as measurements.\n",
    "6. Use the causal inference model to compute the final estimates based on the measurements and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bc3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lbraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, lognorm, gaussian_kde\n",
    "from ipywidgets import interact, FloatSlider, widgets\n",
    "from scipy.special import expit  # Sigmoid function for probability mapping\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4979d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation of Audio-Visual Duration Estimation with Causal Inference Model This notebook simulates the audio-visual duration estimation task using a causal inference model. It allows you to explore how different parameters affect the estimates through interactive controls. ### Experimenter knows the true durations: Standard duration($S_s$) is always 0.5s, test duration varies($S_t$). We simulate and give the parameters to the model, and see how the estimates change. Parameters: - $\\sigma_a$: Standard deviation of auditory measurement noise - $\\sigma_v$: Standard deviation of visual measurement noise - $p_c$: Prior probability of common cause ### Parameters we dont need to adjust specifically: - $c$: Conflict level between auditory and visual stimuli - $\\tmin$ and $\\tmax$: Minimum and maximum test durations ### Steps: 1. Create and repeated array of standard durations: S_s: [0.5, 0.5, ..., 0.5] 2. Create an array of duration differences: delta: [0.0, 0.1, ..., 2.0] 3. Create constant conflict durations: c:[-250,-167,-83,0,83,167,250]ms 4. For each combination of S_s, delta, and c: - Calculate test duration: S_t = S_s + delta + c 5. For simplicity we dont need to estimate noisy measurements, we can directly use the true durations as measurements. 6. Use the causal inference model to compute the final estimates based on the measurements and parameters.\n",
    "# Standard durations of auditory stimuli\n",
    "S_a_s = 0.5  # seconds\n",
    "# repeat\n",
    "S_a_s = np.repeat(S_a_s, 9990)\n",
    "# Range of test durations (difference from standard) -100% to +100%\n",
    "delta_percent = np.linspace(-0.95, 0.95, 7)  # -100% to +100%\n",
    "# Convert percentage differences to absolute time differences (in seconds)\n",
    "delta = delta_percent * S_a_s[0]  # since S_a_s is constant\n",
    "# Constant conflict levels (in seconds)\n",
    "c = np.array([-0.45,-0.35, -0.25, -0.167, -0.083, 0.0, 0.083, 0.167, 0.25, 0.35,+0.45])  # seconds\n",
    "\n",
    "# Ensure all arrays have the same length\n",
    "min_length = min(len(S_a_s), len(delta) * (len(S_a_s) // len(delta)), len(c) * (len(S_a_s) // len(c)))\n",
    "S_a_s = S_a_s[:min_length]\n",
    "delta = np.tile(delta, int(np.ceil(len(S_a_s) / len(delta))))[:min_length]\n",
    "c = np.tile(c, int(np.ceil(len(S_a_s) / len(c))))[:min_length]\n",
    "\n",
    "# Calculate test durations\n",
    "S_a_t = S_a_s + delta \n",
    "S_v_s = S_a_s + c  # Visual test durations with conflict\n",
    "S_v_t = S_a_t # no conflict in test durations for visual\n",
    "# Combine standard and test durations, delta, and conflict into a single dataset\n",
    "simData=np.column_stack((S_a_s, S_a_t, S_v_s, S_v_t, delta, c))\n",
    "# Columns: [S_a_s, S_a_t, S_v_s, S_v_t, delta, c]\n",
    "\n",
    "# create a dataframe\n",
    "import pandas as pd\n",
    "simData = pd.DataFrame(simData, columns=['S_a_s', 'S_a_t', 'S_v_s', 'S_v_t', 'delta', 'c'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a479f150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S_a_s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S_a_t",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S_v_s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S_v_t",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "delta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "c",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cccf63f0-742a-4784-a476-a985c233f0cc",
       "rows": [
        [
         "0",
         "0.5",
         "0.025000000000000022",
         "0.04999999999999999",
         "0.025000000000000022",
         "-0.475",
         "-0.45"
        ],
        [
         "1",
         "0.5",
         "0.18333333333333335",
         "0.15000000000000002",
         "0.18333333333333335",
         "-0.31666666666666665",
         "-0.35"
        ],
        [
         "2",
         "0.5",
         "0.3416666666666667",
         "0.25",
         "0.3416666666666667",
         "-0.15833333333333333",
         "-0.25"
        ],
        [
         "3",
         "0.5",
         "0.5",
         "0.33299999999999996",
         "0.5",
         "0.0",
         "-0.167"
        ],
        [
         "4",
         "0.5",
         "0.6583333333333333",
         "0.417",
         "0.6583333333333333",
         "0.15833333333333333",
         "-0.083"
        ],
        [
         "5",
         "0.5",
         "0.8166666666666667",
         "0.5",
         "0.8166666666666667",
         "0.31666666666666665",
         "0.0"
        ],
        [
         "6",
         "0.5",
         "0.975",
         "0.583",
         "0.975",
         "0.475",
         "0.083"
        ],
        [
         "7",
         "0.5",
         "0.025000000000000022",
         "0.667",
         "0.025000000000000022",
         "-0.475",
         "0.167"
        ],
        [
         "8",
         "0.5",
         "0.18333333333333335",
         "0.75",
         "0.18333333333333335",
         "-0.31666666666666665",
         "0.25"
        ],
        [
         "9",
         "0.5",
         "0.3416666666666667",
         "0.85",
         "0.3416666666666667",
         "-0.15833333333333333",
         "0.35"
        ],
        [
         "10",
         "0.5",
         "0.5",
         "0.95",
         "0.5",
         "0.0",
         "0.45"
        ],
        [
         "11",
         "0.5",
         "0.6583333333333333",
         "0.04999999999999999",
         "0.6583333333333333",
         "0.15833333333333333",
         "-0.45"
        ],
        [
         "12",
         "0.5",
         "0.8166666666666667",
         "0.15000000000000002",
         "0.8166666666666667",
         "0.31666666666666665",
         "-0.35"
        ],
        [
         "13",
         "0.5",
         "0.975",
         "0.25",
         "0.975",
         "0.475",
         "-0.25"
        ],
        [
         "14",
         "0.5",
         "0.025000000000000022",
         "0.33299999999999996",
         "0.025000000000000022",
         "-0.475",
         "-0.167"
        ],
        [
         "15",
         "0.5",
         "0.18333333333333335",
         "0.417",
         "0.18333333333333335",
         "-0.31666666666666665",
         "-0.083"
        ],
        [
         "16",
         "0.5",
         "0.3416666666666667",
         "0.5",
         "0.3416666666666667",
         "-0.15833333333333333",
         "0.0"
        ],
        [
         "17",
         "0.5",
         "0.5",
         "0.583",
         "0.5",
         "0.0",
         "0.083"
        ],
        [
         "18",
         "0.5",
         "0.6583333333333333",
         "0.667",
         "0.6583333333333333",
         "0.15833333333333333",
         "0.167"
        ],
        [
         "19",
         "0.5",
         "0.8166666666666667",
         "0.75",
         "0.8166666666666667",
         "0.31666666666666665",
         "0.25"
        ],
        [
         "20",
         "0.5",
         "0.975",
         "0.85",
         "0.975",
         "0.475",
         "0.35"
        ],
        [
         "21",
         "0.5",
         "0.025000000000000022",
         "0.95",
         "0.025000000000000022",
         "-0.475",
         "0.45"
        ],
        [
         "22",
         "0.5",
         "0.18333333333333335",
         "0.04999999999999999",
         "0.18333333333333335",
         "-0.31666666666666665",
         "-0.45"
        ],
        [
         "23",
         "0.5",
         "0.3416666666666667",
         "0.15000000000000002",
         "0.3416666666666667",
         "-0.15833333333333333",
         "-0.35"
        ],
        [
         "24",
         "0.5",
         "0.5",
         "0.25",
         "0.5",
         "0.0",
         "-0.25"
        ],
        [
         "25",
         "0.5",
         "0.6583333333333333",
         "0.33299999999999996",
         "0.6583333333333333",
         "0.15833333333333333",
         "-0.167"
        ],
        [
         "26",
         "0.5",
         "0.8166666666666667",
         "0.417",
         "0.8166666666666667",
         "0.31666666666666665",
         "-0.083"
        ],
        [
         "27",
         "0.5",
         "0.975",
         "0.5",
         "0.975",
         "0.475",
         "0.0"
        ],
        [
         "28",
         "0.5",
         "0.025000000000000022",
         "0.583",
         "0.025000000000000022",
         "-0.475",
         "0.083"
        ],
        [
         "29",
         "0.5",
         "0.18333333333333335",
         "0.667",
         "0.18333333333333335",
         "-0.31666666666666665",
         "0.167"
        ],
        [
         "30",
         "0.5",
         "0.3416666666666667",
         "0.75",
         "0.3416666666666667",
         "-0.15833333333333333",
         "0.25"
        ],
        [
         "31",
         "0.5",
         "0.5",
         "0.85",
         "0.5",
         "0.0",
         "0.35"
        ],
        [
         "32",
         "0.5",
         "0.6583333333333333",
         "0.95",
         "0.6583333333333333",
         "0.15833333333333333",
         "0.45"
        ],
        [
         "33",
         "0.5",
         "0.8166666666666667",
         "0.04999999999999999",
         "0.8166666666666667",
         "0.31666666666666665",
         "-0.45"
        ],
        [
         "34",
         "0.5",
         "0.975",
         "0.15000000000000002",
         "0.975",
         "0.475",
         "-0.35"
        ],
        [
         "35",
         "0.5",
         "0.025000000000000022",
         "0.25",
         "0.025000000000000022",
         "-0.475",
         "-0.25"
        ],
        [
         "36",
         "0.5",
         "0.18333333333333335",
         "0.33299999999999996",
         "0.18333333333333335",
         "-0.31666666666666665",
         "-0.167"
        ],
        [
         "37",
         "0.5",
         "0.3416666666666667",
         "0.417",
         "0.3416666666666667",
         "-0.15833333333333333",
         "-0.083"
        ],
        [
         "38",
         "0.5",
         "0.5",
         "0.5",
         "0.5",
         "0.0",
         "0.0"
        ],
        [
         "39",
         "0.5",
         "0.6583333333333333",
         "0.583",
         "0.6583333333333333",
         "0.15833333333333333",
         "0.083"
        ],
        [
         "40",
         "0.5",
         "0.8166666666666667",
         "0.667",
         "0.8166666666666667",
         "0.31666666666666665",
         "0.167"
        ],
        [
         "41",
         "0.5",
         "0.975",
         "0.75",
         "0.975",
         "0.475",
         "0.25"
        ],
        [
         "42",
         "0.5",
         "0.025000000000000022",
         "0.85",
         "0.025000000000000022",
         "-0.475",
         "0.35"
        ],
        [
         "43",
         "0.5",
         "0.18333333333333335",
         "0.95",
         "0.18333333333333335",
         "-0.31666666666666665",
         "0.45"
        ],
        [
         "44",
         "0.5",
         "0.3416666666666667",
         "0.04999999999999999",
         "0.3416666666666667",
         "-0.15833333333333333",
         "-0.45"
        ],
        [
         "45",
         "0.5",
         "0.5",
         "0.15000000000000002",
         "0.5",
         "0.0",
         "-0.35"
        ],
        [
         "46",
         "0.5",
         "0.6583333333333333",
         "0.25",
         "0.6583333333333333",
         "0.15833333333333333",
         "-0.25"
        ],
        [
         "47",
         "0.5",
         "0.8166666666666667",
         "0.33299999999999996",
         "0.8166666666666667",
         "0.31666666666666665",
         "-0.167"
        ],
        [
         "48",
         "0.5",
         "0.975",
         "0.417",
         "0.975",
         "0.475",
         "-0.083"
        ],
        [
         "49",
         "0.5",
         "0.025000000000000022",
         "0.5",
         "0.025000000000000022",
         "-0.475",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 9988
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_a_s</th>\n",
       "      <th>S_a_t</th>\n",
       "      <th>S_v_s</th>\n",
       "      <th>S_v_t</th>\n",
       "      <th>delta</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>-0.475000</td>\n",
       "      <td>-0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>-0.316667</td>\n",
       "      <td>-0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>-0.158333</td>\n",
       "      <td>-0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>-0.316667</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>-0.158333</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9988 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      S_a_s     S_a_t  S_v_s     S_v_t     delta      c\n",
       "0       0.5  0.025000  0.050  0.025000 -0.475000 -0.450\n",
       "1       0.5  0.183333  0.150  0.183333 -0.316667 -0.350\n",
       "2       0.5  0.341667  0.250  0.341667 -0.158333 -0.250\n",
       "3       0.5  0.500000  0.333  0.500000  0.000000 -0.167\n",
       "4       0.5  0.658333  0.417  0.658333  0.158333 -0.083\n",
       "...     ...       ...    ...       ...       ...    ...\n",
       "9983    0.5  0.183333  0.583  0.183333 -0.316667  0.083\n",
       "9984    0.5  0.341667  0.667  0.341667 -0.158333  0.167\n",
       "9985    0.5  0.500000  0.750  0.500000  0.000000  0.250\n",
       "9986    0.5  0.658333  0.850  0.658333  0.158333  0.350\n",
       "9987    0.5  0.816667  0.950  0.816667  0.316667  0.450\n",
       "\n",
       "[9988 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf04c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "S_a_t min: 0.025, max: 0.975\n",
      "S_v_t min: 0.025, max: 0.975\n",
      "S_v_s min: 0.050, max: 0.950\n",
      "\n",
      "Negative/zero durations:\n",
      "S_a_t <= 0: 0 cases\n",
      "S_v_t <= 0: 0 cases\n",
      "S_v_s <= 0: 0 cases\n"
     ]
    }
   ],
   "source": [
    "# Check for potential issues with negative durations\n",
    "print(\"Data summary:\")\n",
    "print(f\"S_a_t min: {simData['S_a_t'].min():.3f}, max: {simData['S_a_t'].max():.3f}\")\n",
    "print(f\"S_v_t min: {simData['S_v_t'].min():.3f}, max: {simData['S_v_t'].max():.3f}\")\n",
    "print(f\"S_v_s min: {simData['S_v_s'].min():.3f}, max: {simData['S_v_s'].max():.3f}\")\n",
    "\n",
    "# Check if any values are negative or zero\n",
    "negative_Sa_t = simData[simData['S_a_t'] <= 0]\n",
    "negative_Sv_t = simData[simData['S_v_t'] <= 0] \n",
    "negative_Sv_s = simData[simData['S_v_s'] <= 0]\n",
    "\n",
    "print(f\"\\nNegative/zero durations:\")\n",
    "print(f\"S_a_t <= 0: {len(negative_Sa_t)} cases\")\n",
    "print(f\"S_v_t <= 0: {len(negative_Sv_t)} cases\") \n",
    "print(f\"S_v_s <= 0: {len(negative_Sv_s)} cases\")\n",
    "\n",
    "if len(negative_Sv_s) > 0:\n",
    "    print(f\"Example negative S_v_s: {negative_Sv_s[['S_v_s', 'c']].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "390e6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUSION MODEL IMPLEMENTATION ---\n",
    "def fusion_estimate(m_a, m_v, sigma_a, sigma_v):\n",
    "    \"\"\"Bayesian optimal integration of auditory and visual measurements.\"\"\"\n",
    "    # Calculate weights based on reliabilities (inverse variances)\n",
    "    w_a = 1 / sigma_a**2\n",
    "    w_v = 1 / sigma_v**2\n",
    "    # Compute the fused estimate\n",
    "    fused_estimate = (w_a * m_a + w_v * m_v) / (w_a + w_v)\n",
    "    return fused_estimate\n",
    "\n",
    "# --- CAUSAL INFERENCE MODEL IMPLEMENTATION ---\n",
    "def p_single(m,sigma,t_min,t_max):\n",
    "    \"\"\"p(m | C=2) and Gaussian measurement noise N(m; y, sigma^2).\"\"\"\n",
    "    hi_cdf= norm.cdf((t_max - m) /sigma)\n",
    "    lo_cdf=norm.cdf((t_min-m)/sigma)\n",
    "    return (hi_cdf-lo_cdf)/(t_max-t_min)\n",
    "\n",
    "# Causal inference model for duration estimation\n",
    "def L_C1(m_a,m_v,sigma_a,sigma_v,t_min,t_max):\n",
    "    sigma_c_sq = (sigma_a**2 * sigma_v**2) / (sigma_a**2 + sigma_v**2)\n",
    "    sigma_c = np.sqrt(sigma_c_sq)\n",
    "    mu_c = (m_a / sigma_a**2 + m_v / sigma_v**2) / (1 / sigma_a**2 + 1 / sigma_v**2)\n",
    "\n",
    "    hi_cdf = norm.cdf((t_max-mu_c)/sigma_c)\n",
    "    lo_cdf = norm.cdf((t_min-mu_c)/sigma_c)\n",
    "    \n",
    "    expo = np.exp(-(m_a-m_v)**2/(2*(sigma_a**2+sigma_v**2)))\n",
    "    \n",
    "    prior = 1/(t_max-t_min)\n",
    "    return prior * sigma_c/np.sqrt(sigma_a**2 * sigma_v**2) * (hi_cdf-lo_cdf) * expo\n",
    "\n",
    "def L_C2(m_a,m_v,sigma_a,sigma_v,t_min,t_max):\n",
    "    \"\"\" Likelihood of separate sources: product of two marginal likelihoods \n",
    "        two integral over two hidden duration y_a y_v\"\"\"\n",
    "    return p_single(m_a,sigma_a,t_min,t_max) * p_single(m_v,sigma_v,t_min,t_max)\n",
    "\n",
    "def posterior_C1(m_a,m_v,sigma_a,sigma_v,p_c,t_min,t_max):\n",
    "    \"\"\" Posterior probability of common cause P(C=1 | m_a,m_v) \"\"\"\n",
    "    # Likelihoods under each causal structure\n",
    "    L1 = L_C1(m_a,m_v,sigma_a,sigma_v,t_min,t_max)\n",
    "    L2 = L_C2(m_a,m_v,sigma_a,sigma_v,t_min,t_max)\n",
    "    # Unnormalized posteriors\n",
    "    post_C1_unnorm = L1 * p_c\n",
    "    post_C2_unnorm = L2 * (1 - p_c)\n",
    "    # Normalization constant\n",
    "    norm_const = post_C1_unnorm + post_C2_unnorm\n",
    "    \n",
    "    # Handle numerical stability\n",
    "    if np.isscalar(norm_const):\n",
    "        if norm_const == 0:\n",
    "            return 0.5\n",
    "    else:\n",
    "        norm_const = np.where(norm_const == 0, 1e-10, norm_const)\n",
    "    \n",
    "    # Posterior probabilities\n",
    "    post_C1 = post_C1_unnorm / norm_const\n",
    "    return post_C1\n",
    "\n",
    "def causal_inference_estimate(m_a, m_v, sigma_a, sigma_v, p_c, t_min, t_max, model):\n",
    "    \"\"\"Causal inference duration estimate (stays in current space - log or linear)\"\"\"\n",
    "    # Posterior probability of common cause\n",
    "    p_C1 = posterior_C1(m_a, m_v, sigma_a, sigma_v, p_c, t_min, t_max)\n",
    "    \n",
    "    # Estimate under common cause (fused estimate)\n",
    "    est_C1 = fusion_estimate(m_a, m_v, sigma_a, sigma_v)\n",
    "    # Estimate under separate causes (auditory estimate)\n",
    "    est_C2 = m_a\n",
    "    \n",
    "    # Final estimate as a weighted average\n",
    "    final_estimate = p_C1 * est_C1 + (1 - p_C1) * est_C2\n",
    "    \n",
    "    if model == \"log-space\":\n",
    "        final_estimate = p_C1 * est_C1 + (1 - p_C1) * est_C2\n",
    "\n",
    "    return final_estimate\n",
    "\n",
    "def forced_fusion_estimate(m_a, m_v, sigma_a, sigma_v, model):\n",
    "    \"\"\"Forced fusion model - always fuses, no causal inference\"\"\"\n",
    "    fused = fusion_estimate(m_a, m_v, sigma_a, sigma_v)\n",
    "    \n",
    "    if model == \"log-space\":\n",
    "        # Convert back to linear space\n",
    "        return np.exp(fused)\n",
    "    else:\n",
    "        return fused\n",
    "\n",
    "def estimate_duration(m_a,m_v,sigma_a,sigma_v,p_c,t_min,t_max,lambda_=0.1, model=\"linear-space\"):\n",
    "    \"\"\" Final duration estimate as a weighted average of estimates under each causal structure \"\"\"\n",
    "    \n",
    "    if model == \"log-space\":\n",
    "        # For log-space model: measurements are in log space, causal inference in log space\n",
    "        # t_min and t_max should also be in log space\n",
    "        log_t_min = np.log(t_min)\n",
    "        log_t_max = np.log(t_max)        \n",
    "        # Do causal inference in log space\n",
    "        final_estimate = causal_inference_estimate(m_a, m_v, sigma_a, sigma_v, p_c, log_t_min, log_t_max, model=model)\n",
    "    else:  # linear-space\n",
    "        # For linear-space model: everything in linear space\n",
    "        final_estimate = causal_inference_estimate(m_a, m_v, sigma_a, sigma_v, p_c, t_min, t_max, model=model)\n",
    "    \n",
    "    return final_estimate\n",
    "\n",
    "def estimate_probability_matching(m_a, m_v, sigma_a, sigma_v, p_c, t_min, t_max, model=\"linear-space\"):\n",
    "    \"\"\" Probability matching model - sample from the posterior distribution \"\"\"\n",
    "    if model == \"log-space\":\n",
    "        t_min = np.log(t_min)\n",
    "        t_max = np.log(t_max)\n",
    "    # Calculate posterior probability of common cause\n",
    "    post_C1 = posterior_C1(m_a, m_v, sigma_a, sigma_v, p_c, t_min, t_max)\n",
    "    est_fused = fusion_estimate(m_a, m_v, sigma_a, sigma_v)\n",
    "    est_separate = m_a\n",
    "    # bernoulli sampling based on posterior\n",
    "    from scipy.stats import bernoulli\n",
    "    b = bernoulli(post_C1).rvs()\n",
    "    \n",
    "    # Probability matching estimate based on sampled causal structure\n",
    "    final_estimate = b * est_fused + (1 - b) * est_separate\n",
    "    \n",
    "    if model == \"log-space\":\n",
    "        final_estimate = np.exp(final_estimate)\n",
    "    \n",
    "    return final_estimate\n",
    "\n",
    "    \n",
    "\n",
    "# Calculate probabilities of choosing test longer than standard for each condition\n",
    "def prob_test_longer_condition(S_a_s, S_a_t, S_v_s, S_v_t, sigma_a, sigma_v, p_c, t_min, t_max, lambda_=0.1, measurement=\"linear-space\",model=\"lognorm\", nSimul=1000, use_forced_fusion=False):\n",
    "    \"\"\" Calculate the probability of choosing the test duration as longer than the standard duration for a specific condition. \"\"\"\n",
    "    \n",
    "    # Check for invalid durations (≤ 0) when using log-space\n",
    "    if measurement == \"log-space\":\n",
    "        min_duration = 1e-6  # Very small positive number\n",
    "        S_a_s = max(S_a_s, min_duration)\n",
    "        S_a_t = max(S_a_t, min_duration)\n",
    "        S_v_s = max(S_v_s, min_duration)\n",
    "        S_v_t = max(S_v_t, min_duration)\n",
    "    \n",
    "    # Generate measurements with noise\n",
    "    if measurement == \"log-space\":\n",
    "        # Measurements are Gaussian in log space (like your main implementation)\n",
    "        m_a_s = np.random.normal(np.log(S_a_s), scale=sigma_a, size=nSimul)\n",
    "        m_a_t = np.random.normal(np.log(S_a_t), scale=sigma_a, size=nSimul)\n",
    "        m_v_s = np.random.normal(np.log(S_v_s), scale=sigma_v, size=nSimul)\n",
    "        m_v_t = np.random.normal(np.log(S_v_t), scale=sigma_v, size=nSimul)\n",
    "    elif measurement == \"linear-space\":\n",
    "        m_a_s = np.random.normal(S_a_s, scale=sigma_a, size=nSimul)\n",
    "        m_a_t = np.random.normal(S_a_t, scale=sigma_a, size=nSimul)\n",
    "        m_v_s = np.random.normal(S_v_s, scale=sigma_v, size=nSimul)\n",
    "        m_v_t = np.random.normal(S_v_t, scale=sigma_v, size=nSimul)\n",
    "\n",
    "\n",
    "    \n",
    "    if model==\"ForcedFusion\":\n",
    "        # Forced fusion: always integrate, no causal inference\n",
    "        est_standard = forced_fusion_estimate(m_a_s, m_v_s, sigma_a, sigma_v, model)\n",
    "        est_test = forced_fusion_estimate(m_a_t, m_v_t, sigma_a, sigma_v, model)\n",
    "    elif model==\"CausalInference\":\n",
    "        # Causal inference model\n",
    "        est_standard = estimate_duration(m_a_s, m_v_s, sigma_a, sigma_v, p_c, t_min, t_max, lambda_, model)\n",
    "        est_test = estimate_duration(m_a_t, m_v_t, sigma_a, sigma_v, p_c, t_min, t_max, lambda_, model)\n",
    "    elif model==\"ProbablityMatching\":\n",
    "        # Probability matching model\n",
    "        est_standard = estimate_probability_matching(m_a_s, m_v_s, sigma_a, sigma_v, p_c, t_min, t_max, model)\n",
    "        est_test = estimate_probability_matching(m_a_t, m_v_t, sigma_a, sigma_v, p_c, t_min, t_max, model)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model specified. Choose 'ForcedFusion', 'CausalInference', or 'ProbablityMatching'.\")\n",
    "    # Calculate probability\n",
    "    p_base = np.mean(est_test > est_standard)\n",
    "    p_final = (1 - lambda_) * p_base + lambda_ * 0.5\n",
    "    return p_final\n",
    "\n",
    "\n",
    "# Updated simulation function that generates psychometric curves\n",
    "def simulate_duration_estimation(sigma_a=0.05, sigma_v=0.1, p_c=0.5, lambda_=0.1,measurement=\"linear-space\", model=\"lognorm\", nSimul=1000, t_min=0.01, t_max=3.0):\n",
    "    \"\"\"\n",
    "    Simulate duration estimation and generate psychometric curves for different conflict levels.\n",
    "    \"\"\"\n",
    "    # Get unique conditions\n",
    "    unique_deltas = np.sort(simData['delta'].unique())\n",
    "    unique_conflicts = np.sort(simData['c'].unique())\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    results_fusion = []  # For forced fusion model\n",
    "    \n",
    "    print(f\"Simulating with {len(unique_deltas)} deltas and {len(unique_conflicts)} conflict levels...\")\n",
    "    print(f\"Model: {model}\")\n",
    "    \n",
    "    # For each conflict level\n",
    "    for conflict in unique_conflicts:\n",
    "        conflict_results = []\n",
    "        conflict_results_fusion = []\n",
    "        \n",
    "        # For each delta (test duration difference)\n",
    "        for delta in unique_deltas:\n",
    "            # Get the stimulus durations for this condition\n",
    "            S_a_s = 0.5  # Standard auditory duration\n",
    "            S_a_t = S_a_s + delta  # Test auditory duration\n",
    "            S_v_s = S_a_s + conflict  # Standard visual duration (with conflict)\n",
    "            S_v_t = S_a_t  # Test visual duration (same as auditory test)\n",
    "            \n",
    "            # Skip conditions with non-positive durations for log-space\n",
    "            if model == \"log-space\" and (S_a_t <= 0 or S_v_t <= 0):\n",
    "                print(f\"Skipping condition: delta={delta:.3f}, conflict={conflict:.3f} (negative duration)\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate probability for causal inference model\n",
    "            p_longer = prob_test_longer_condition(\n",
    "                S_a_s, S_a_t, S_v_s, S_v_t, \n",
    "                sigma_a, sigma_v, p_c, t_min, t_max, lambda_, measurement, model, nSimul, use_forced_fusion=False\n",
    "            )\n",
    "            \n",
    "            # Calculate probability for forced fusion model\n",
    "            p_longer_fusion = prob_test_longer_condition(\n",
    "                S_a_s, S_a_t, S_v_s, S_v_t, \n",
    "                sigma_a, sigma_v, p_c, t_min, t_max, lambda_, measurement, model, nSimul, use_forced_fusion=True\n",
    "            )\n",
    "            \n",
    "            conflict_results.append({\n",
    "                'delta': delta,\n",
    "                'delta_ms': delta * 1000,\n",
    "                'conflict': conflict,\n",
    "                'conflict_ms': conflict * 1000,\n",
    "                'p_test_longer': p_longer,\n",
    "                'S_a_s': S_a_s,\n",
    "                'S_a_t': S_a_t,\n",
    "                'S_v_s': S_v_s,\n",
    "                'S_v_t': S_v_t\n",
    "            })\n",
    "            \n",
    "            conflict_results_fusion.append({\n",
    "                'delta': delta,\n",
    "                'delta_ms': delta * 1000,\n",
    "                'conflict': conflict,\n",
    "                'conflict_ms': conflict * 1000,\n",
    "                'p_test_longer': p_longer_fusion,\n",
    "                'S_a_s': S_a_s,\n",
    "                'S_a_t': S_a_t,\n",
    "                'S_v_s': S_v_s,\n",
    "                'S_v_t': S_v_t\n",
    "            })\n",
    "        \n",
    "        results.extend(conflict_results)\n",
    "        results_fusion.extend(conflict_results_fusion)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_fusion_df = pd.DataFrame(results_fusion)\n",
    "    \n",
    "    if len(results_df) == 0:\n",
    "        print(\"No valid conditions found!\")\n",
    "        return results_df\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot 1: Psychometric curves for each conflict level\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_conflicts)))\n",
    "    \n",
    "    # Plot causal inference model (solid lines)\n",
    "    for i, conflict in enumerate(unique_conflicts):\n",
    "        conflict_data = results_df[results_df['conflict'] == conflict]\n",
    "        if len(conflict_data) > 0:\n",
    "            plt.plot(conflict_data['delta_ms'], conflict_data['p_test_longer'], \n",
    "                    'o-', color=colors[i], label=f'c: {conflict*1000:.0f}ms', alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Plot forced fusion model (dashed lines)\n",
    "    for i, conflict in enumerate(unique_conflicts):\n",
    "        conflict_data = results_fusion_df[results_fusion_df['conflict'] == conflict]\n",
    "        if len(conflict_data) > 0:\n",
    "            plt.plot(conflict_data['delta_ms'], conflict_data['p_test_longer'], \n",
    "                    '-', color=colors[i], alpha=0.6, linewidth=0.5)\n",
    "    \n",
    "    plt.title('Psychometric Curves: Causal Inference vs Forced Fusion')\n",
    "    plt.xlabel('Test Duration Difference from Standard (ms)')\n",
    "    plt.ylabel('P(Choosing Test as Longer)')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    # Plot 2: PSE comparison for both models\n",
    "    plt.subplot(1, 2, 2)\n",
    "    pse_by_conflict = []\n",
    "    pse_by_conflict_fusion = []\n",
    "    \n",
    "    # Calculate PSE for causal inference model\n",
    "    for conflict in unique_conflicts:\n",
    "        conflict_data = results_df[results_df['conflict'] == conflict].sort_values('delta_ms')\n",
    "        if len(conflict_data) > 1:\n",
    "            try:\n",
    "                p_values = conflict_data['p_test_longer'].values\n",
    "                delta_values = conflict_data['delta_ms'].values\n",
    "                \n",
    "                if p_values.min() <= 0.5 <= p_values.max():\n",
    "                    pse = np.interp(0.5, p_values, delta_values)\n",
    "                else:\n",
    "                    closest_idx = np.argmin(np.abs(p_values - 0.5))\n",
    "                    pse = delta_values[closest_idx]\n",
    "                \n",
    "                pse_by_conflict.append({\n",
    "                    'conflict_ms': conflict*1000, \n",
    "                    'pse_ms': pse,\n",
    "                    'conflict': conflict\n",
    "                })\n",
    "            except:\n",
    "                pse_by_conflict.append({\n",
    "                    'conflict_ms': conflict*1000, \n",
    "                    'pse_ms': 0,\n",
    "                    'conflict': conflict\n",
    "                })\n",
    "    \n",
    "    # Calculate PSE for forced fusion model\n",
    "    for conflict in unique_conflicts:\n",
    "        conflict_data = results_fusion_df[results_fusion_df['conflict'] == conflict].sort_values('delta_ms')\n",
    "        if len(conflict_data) > 1:\n",
    "            try:\n",
    "                p_values = conflict_data['p_test_longer'].values\n",
    "                delta_values = conflict_data['delta_ms'].values\n",
    "                \n",
    "                if p_values.min() <= 0.5 <= p_values.max():\n",
    "                    pse = np.interp(0.5, p_values, delta_values)\n",
    "                else:\n",
    "                    closest_idx = np.argmin(np.abs(p_values - 0.5))\n",
    "                    pse = delta_values[closest_idx]\n",
    "                \n",
    "                pse_by_conflict_fusion.append({\n",
    "                    'conflict_ms': conflict*1000, \n",
    "                    'pse_ms': pse,\n",
    "                    'conflict': conflict\n",
    "                })\n",
    "            except:\n",
    "                pse_by_conflict_fusion.append({\n",
    "                    'conflict_ms': conflict*1000, \n",
    "                    'pse_ms': 0,\n",
    "                    'conflict': conflict\n",
    "                })\n",
    "    \n",
    "    # Plot both PSE curves\n",
    "    if pse_by_conflict:\n",
    "        pse_df = pd.DataFrame(pse_by_conflict)\n",
    "        plt.plot(pse_df['conflict_ms'], pse_df['pse_ms'], 'ro-', linewidth=2, markersize=8, label='Causal Inference')\n",
    "        \n",
    "    if pse_by_conflict_fusion:\n",
    "        pse_fusion_df = pd.DataFrame(pse_by_conflict_fusion)\n",
    "        plt.plot(pse_fusion_df['conflict_ms'], pse_fusion_df['pse_ms'], 'bs--', linewidth=2, markersize=8, label='Forced Fusion', alpha=0.7)\n",
    "    \n",
    "    plt.title('PSE vs Conflict: Causal Inference vs Forced Fusion')\n",
    "    plt.xlabel('Conflict Level (ms)')\n",
    "    plt.ylabel('PSE (ms from standard)')\n",
    "    plt.ylim(-500, 500)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    # Vertical cut lines at ±250 ms\n",
    "    ax = plt.gca()\n",
    "    ax.axvline(+250, color='black', linestyle='-', alpha=0.3, label='±250ms bounds')\n",
    "    ax.axvline(-250, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "    # Shade regions beyond the ±250 ms bounds\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "\n",
    "    # Shade to the right of +250 ms\n",
    "    if xmax > 250:\n",
    "        ax.axvspan(max(250, xmin), xmax, color='lightgray', alpha=0.2, linewidth=0)\n",
    "\n",
    "    # Shade to the left of -250 ms\n",
    "    if xmin < -250:\n",
    "        ax.axvspan(xmin, min(-250, xmax), color='lightgray', alpha=0.2, linewidth=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the simulation with log-space model (your main model)\n",
    "# print(\"Running causal inference simulation...\")\n",
    "# results = simulate_duration_estimation(sigma_a=0.05, sigma_v=0.1, p_c=0.5, lambda_=0.1, model=\"log-space\", nSimul=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83099d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8f4bcb2c5b40899890b7cb2ffb5b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='σₐ (auditory):', max=1.0, min=0.01, step=0.01), Floa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive widget for parameter exploration\n",
    "def interactive_simulation(sigma_a=0.05, sigma_v=0.1, p_c=0.5, lambda_=0.1, model_space=\"linear-space\", n_simulations=500, t_min=0.01, t_max=3.0, modelName=\"CausalInference\"):\n",
    "    \"\"\"Interactive version of the simulation with parameter controls\"\"\"\n",
    "    results = simulate_duration_estimation(\n",
    "        sigma_a=sigma_a, \n",
    "        sigma_v=sigma_v, \n",
    "        p_c=p_c, \n",
    "        lambda_=lambda_, \n",
    "        measurement=model_space, \n",
    "        nSimul=n_simulations,\n",
    "        t_min=t_min,\n",
    "        t_max=t_max,\n",
    "        model=modelName\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Create interactive widget\n",
    "from ipywidgets import interact, FloatSlider, Dropdown, IntSlider\n",
    "\n",
    "interact(\n",
    "    interactive_simulation,\n",
    "    sigma_a=FloatSlider(value=0.5, min=0.01, max=1, step=0.01, description='σₐ (auditory):'),\n",
    "    sigma_v=FloatSlider(value=0.3, min=0.01, max=0.8, step=0.01, description='σᵥ (visual):'),\n",
    "    p_c=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description='p_c (common cause):'),\n",
    "    lambda_=FloatSlider(value=0.1, min=0.0, max=0.5, step=0.01, description='λ (lapse rate):'),\n",
    "    model_space=Dropdown(options=['linear-space', 'log-space'], value='log-space', description='Measurement space:'),\n",
    "    n_simulations=IntSlider(value=1000, min=100, max=2000, step=100, description='N simulations:'),\n",
    "    t_min=FloatSlider(value=0.01, min=0.001, max=0.5, step=0.001, description='t_min (s):'),\n",
    "    t_max=FloatSlider(value=3.0, min=0.5, max=5.0, step=0.01, description='t_max (s):'),\n",
    "    modelName=Dropdown(options=['CausalInference', 'ForcedFusion', 'ProbablityMatching'], value='CausalInference', description='Model:')\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09813c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
